% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='scriptsize', fig.width=4, fig.height=4)
options(width=60, dev='pdf')
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
      @


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}



% Title page setup
\title[Time Series Univariate]{Time Series Univariate}
\subtitle{FRE6871 \& FRE7241, Fall 2020}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle


%%%%%%%%%%%%%%%
\section{Package \protect\emph{tseries} for Time Series Analysis}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{tseries} for Time Series Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{tseries} contains functions for time series analysis and computational finance, such as:
      \begin{itemize}
        \item downloading historical data,
        \item plotting time series,
        \item calculating risk and performance measures,
        \item statistical \emph{hypothesis testing},
        \item calibrating models to time series,
        \item portfolio optimization,
      \end{itemize}
      Package \emph{tseries} accepts time series of class \texttt{"ts"} and \texttt{"zoo"}, and also has its own class \texttt{"irts"} for irregular spaced time-series objects.
      \vskip1ex
      The package \emph{zoo} is designed for managing \emph{time series} and ordered data objects.
      \vskip1ex
      The function \texttt{zoo::coredata()} extracts the underlying numeric data from a complex data object.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Get documentation for package tseries
packageDescription("tseries")  # Get short description

help(package="tseries")  # Load help page

library(tseries)  # Load package tseries

data(package="tseries")  # List all datasets in "tseries"

ls("package:tseries")  # List all objects in "tseries"

detach("package:tseries")  # Remove tseries from search path
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting \protect\emph{OHLC} Time Series Using Package \protect\emph{tseries}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{tseries} contains functions for plotting time series:
      \begin{itemize}
        \item \texttt{seqplot.ts()} for plotting two time series in same panel.
        \item \texttt{plotOHLC()} for plotting \emph{OHLC} time series.
      \end{itemize}
      The function \texttt{plotOHLC()} from package \emph{tseries} plots \emph{OHLC} time series.
      <<tseries_OHLC,echo=(-(1:1)),eval=FALSE,fig.show='hide'>>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
load(file="C:/Develop/lecture_slides/data/zoo_data.RData")
# Get start and end dates
in_dex <- time(ts_stx_adj)
e_nd <- in_dex[NROW(in_dex)]
st_art <- round((4*e_nd + in_dex[1])/5)
# Plot using plotOHLC
plotOHLC(window(ts_stx_adj,
                start=st_art,
                end=e_nd)[, 1:4],
         xlab="", ylab="")
title(main="MSFT OHLC Prices")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/tseries_OHLC-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Two Time Series Using \protect\emph{tseries}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{seqplot.ts()} from package \emph{tseries} plots two time series in same panel.
      \vskip1ex
      A \emph{ts} time series can be created from a \emph{zoo} time series using the function \emph{ts()}, after extracting the data and date attributes from the \emph{zoo} time series.
      \vskip1ex
      The function \texttt{decimal\_date()} from package \emph{lubridate} converts \texttt{POSIXct} objects into \texttt{numeric} \emph{year-fraction} dates.
      \vspace{-1em}
      <<tseries_seqplot,echo=(-(1:1)),eval=FALSE,fig.show='hide'>>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
library(lubridate)  # Load lubridate
# Get start and end dates of ms_ft
start_date <- decimal_date(start(ms_ft))
end_date <- decimal_date(end(ms_ft))
# Calculate frequency of ms_ft
fre_quency <-
  NROW(ms_ft)/(end_date-start_date)
# Extract data from ms_ft
da_ta <- zoo::coredata(
  window(ms_ft, start=as.Date("2015-01-01"),
         end=end(ms_ft)))
# Create ts object using ts()
ts_stx <- ts(data=da_ta,
  start=decimal_date(as.Date("2015-01-01")),
  frequency=fre_quency)
seqplot.ts(x=ts_stx[, 1], y=ts_stx[, 4], xlab="", ylab="")
title(main="MSFT Open and Close Prices", line=-1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/tseries_seqplot-1}\\
      \vspace{-3em}
      The function \texttt{zoo::coredata()} extracts the underlying numeric data from a complex data object.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk and Performance Estimation Using \protect\emph{tseries}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{tseries} contains functions for calculating risk and performance:
      \begin{itemize}
        \item \texttt{maxdrawdown()} for calculating the maximum drawdown,
        \item \texttt{sharpe()} for calculating the \emph{Sharpe} ratio (defined as the excess return divided by the standard deviation),
        \item \texttt{sterling()} for calculating the \emph{Sterling} ratio (defined as the return divided by the maximum drawdown),
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(tseries)  # Load package tseries
# Calculate maximum drawdown
maxdrawdown(msft_adj[, "AdjClose"])
max_drawd <- maxdrawdown(msft_adj[, "AdjClose"])
index(msft_adj)[max_drawd$from]
index(msft_adj)[max_drawd$to]
# Calculate Sharpe ratio
sharpe(msft_adj[, "AdjClose"])
# Calculate Sterling ratio
sterling(as.numeric(msft_adj[, "AdjClose"]))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hypothesis Testing Using \protect\emph{tseries}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{tseries} contains functions for testing statistical hypothesis on time series:
      \begin{itemize}
        \item \texttt{jarque.bera.test()} \emph{Jarque-Bera} test for normality of distribution of returns,
        \item \texttt{adf.test()} \emph{Augmented Dickey-Fuller} test for existence of unit roots,
        \item \texttt{pp.test()} \emph{Phillips-Perron} test for existence of unit roots,
        \item \texttt{kpss.test()} \emph{KPSS} test for stationarity,
        \item \texttt{po.test()} \emph{Phillips-Ouliaris} test for cointegration,
        \item \texttt{bds.test()} \emph{BDS} test for randomness,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
ms_ft <- suppressWarnings(  # Load MSFT data
  get.hist.quote(instrument="MSFT",
                 start=Sys.Date()-365,
                 end=Sys.Date(),
                 origin="1970-01-01")
)  # end suppressWarnings
class(ms_ft)
dim(ms_ft)
tail(ms_ft, 4)

# Calculate Sharpe ratio
sharpe(ms_ft[, "Close"], r=0.01)
# Add title
plot(ms_ft[, "Close"], xlab="", ylab="")
title(main="MSFT Close Prices", line=-1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calibrating Time Series Models Using \protect\emph{tseries}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{tseries} contains functions for calibrating models to time series:
      \begin{itemize}
        \item \texttt{garch()} for calibrating \emph{GARCH} volatility models,
        \item \texttt{arma()} for calibrating \texttt{ARMA} models,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(tseries)  # Load package tseries
ms_ft <- suppressWarnings(  # Load MSFT data
  get.hist.quote(instrument="MSFT",
                 start=Sys.Date()-365,
                 end=Sys.Date(),
                 origin="1970-01-01")
)  # end suppressWarnings
class(ms_ft)
dim(ms_ft)
tail(ms_ft, 4)

# Calculate Sharpe ratio
sharpe(ms_ft[, "Close"], r=0.01)
# Add title
plot(ms_ft[, "Close"], xlab="", ylab="")
title(main="MSFT Close Prices", line=-1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Using \protect\emph{tseries}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{tseries} contains functions for miscellaneous functions:
      \vskip1ex
      \texttt{portfolio.optim()} for calculating mean-variance efficient portfolios.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(tseries)  # Load package tseries
ms_ft <- suppressWarnings(  # Load MSFT data
  get.hist.quote(instrument="MSFT",
                 start=Sys.Date()-365,
                 end=Sys.Date(),
                 origin="1970-01-01")
)  # end suppressWarnings
class(ms_ft)
dim(ms_ft)
tail(ms_ft, 4)

# Calculate Sharpe ratio
sharpe(ms_ft[, "Close"], r=0.01)
# Add title
plot(ms_ft[, "Close"], xlab="", ylab="")
title(main="MSFT Close Prices", line=-1)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Package \protect\emph{quantmod} for Quantitative Financial Modeling}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{quantmod} for Quantitative Financial Modeling}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{quantmod} is designed for downloading, manipulating, and visualizing \emph{OHLC} time series data.
      \vskip1ex
      \emph{quantmod} operates on time series of class \texttt{"xts"}, and provides many useful functions for building quantitative financial models:
      \begin{itemize}
        \item \texttt{getSymbols()} for downloading data from external sources (\emph{Yahoo}, \emph{FRED}, etc.),
        \item \texttt{getFinancials()} for downloading financial statements,
        \item \texttt{adjustOHLC()} for adjusting \emph{OHLC} data,
        \item \texttt{Op()}, \texttt{Ad()}, \texttt{Vo()}, etc. for extracting \emph{OHLC} data columns,
        \item \texttt{periodReturn()}, \texttt{dailyReturn()}, etc. for calculating periodic returns,
        \item \texttt{chartSeries()} for candlestick plots of \emph{OHLC} data,
        \item \texttt{addBBands()}, \texttt{addMA()}, \texttt{addVo()}, etc. for adding technical indicators (Moving Averages, Bollinger Bands) and volume data to a plot,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load package quantmod
library(quantmod)
# Get documentation for package quantmod
# Get short description
packageDescription("quantmod")
# Load help page
help(package="quantmod")
# List all datasets in "quantmod"
data(package="quantmod")
# List all objects in "quantmod"
ls("package:quantmod")
# Remove quantmod from search path
detach("package:quantmod")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting \protect\emph{OHLC} Time Series Using \texttt{chartSeries()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chartSeries()} from package \emph{quantmod} can produce a variety of plots for \emph{OHLC} time series, including candlestick plots, bar plots, and line plots.
      \vskip1ex
      The argument \texttt{"type"} determines the type of plot (candlesticks, bars, or lines).
      \vskip1ex
      The argument \texttt{"theme"} accepts a \texttt{"chart.theme"} object, containing parameters that determine the plot appearance (colors, size, fonts).
      \vskip1ex
      \texttt{chartSeries()} automatically plots the volume data in a separate panel.
      \vskip1ex
      \emph{Candlestick} plots are designed to visualize \emph{OHLC} time series.
      <<chartSeries_basic,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(quantmod)
# Plot OHLC candlechart with volume
chartSeries(etf_env$VTI["2014-11"],
            name="VTI",
            theme=chartTheme("white"))
# Plot OHLC bar chart with volume
chartSeries(etf_env$VTI["2014-11"],
            type="bars",
            name="VTI",
            theme=chartTheme("white"))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/chartSeries_basic-1}\\
      Each \emph{candlestick} displays one period of data, and consists of a box representing the \emph{Open} and \emph{Close} prices, and a vertical line representing the \emph{High} and \emph{Low} prices.
      \vskip1ex
      The color of the box signifies whether the \emph{Close} price was higher or lower than the \emph{Open},
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Redrawing Plots Using \texttt{reChart()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{reChart()} redraws plots using the same data set, but using additional parameters that control the plot appearance.
      \vskip1ex
      The argument \texttt{"subset"} allows subsetting the data to a smaller range of dates.
      <<echo=(-(1:1)),eval=FALSE>>=
library(quantmod)
# Plot OHLC candlechart with volume
chartSeries(etf_env$VTI["2008-11/2009-04"],
            name="VTI")
# Redraw plot only for Feb-2009, with white theme
reChart(subset="2009-02",
        theme=chartTheme("white"))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/chartSeries_black.png}\\
      \includegraphics[width=0.5\paperwidth]{figure/chartSeries_white.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Technical Indicators Using \texttt{chartSeries()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The argument \texttt{"TA"} allows adding technical indicators to the plot.
      \vskip1ex
      The technical indicators are functions provided by the package \emph{TTR}.
      \vskip1ex
      The function \texttt{newTA()} allows defining new technical indicators.
      <<chartSeries_TA,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(quantmod)
# Candlechart with Bollinger Bands
chartSeries(etf_env$VTI["2014"],
            TA="addBBands(): addBBands(draw='percent'): addVo()",
            name="VTI with Bollinger Bands",
            theme=chartTheme("white"))
# Candlechart with two Moving Averages
chartSeries(etf_env$VTI["2014"],
            TA="addVo(): addEMA(10): addEMA(30)",
            name="VTI with Moving Averages",
            theme=chartTheme("white"))
# Candlechart with Commodity Channel Index
chartSeries(etf_env$VTI["2014"],
            TA="addVo(): addBBands(): addCCI()",
            name="VTI with Technical Indicators",
            theme=chartTheme("white"))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/chartSeries_TA-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Adding Indicators and Lines Using \texttt{addTA()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{addTA()} adds indicators and lines to plots, and allows plotting lines representing a single vector of data.
      \vskip1ex
      The \texttt{addTA()} function argument \texttt{"on"} determines on which plot panel (subplot) the indicator is drawn.
      \vskip1ex
      \texttt{"on=NA"} is the default, and draws in a new plot panel below the existing plot.
      \vskip1ex
      \texttt{"on=1"} draws in the foreground of the main plot panel, and \texttt{"on=-1"} draws in the background.
      <<chartSeries_addTA,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(quantmod)
library(TTR)
oh_lc <- rutils::etf_env$VTI["2009-02/2009-03"]
VTI_close <- quantmod::Cl(oh_lc); VTI_vol <- quantmod::Vo(oh_lc)
# Calculate volume-weighted average price
v_wap <- TTR::VWAP(price=VTI_close,
      volume=VTI_vol, n=10)
# Plot OHLC candlechart with volume
chartSeries(oh_lc, name="VTI plus VWAP",
            theme=chartTheme("white"))
# Add VWAP to main plot
addTA(ta=v_wap, on=1, col='red')
# Add price minus VWAP in extra panel
addTA(ta=(VTI_close-v_wap), col='red')
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/chartSeries_addTA-3}\\
      The function \texttt{VWAP()} from package \emph{TTR} calculates the Volume Weighted Average Price as the average of past prices multiplied by their trading volumes, divided by the total volume.
      \vskip1ex
      The argument \texttt{"n"} represents the number of look-back periods used for averaging,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Shading Plots Using \texttt{addTA()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{addTA()} accepts Boolean vectors for shading of plots.
      \vskip1ex
      The function \texttt{addLines()} draws vertical or horizontal lines in plots.
      <<chartSeries_addTA_shade,echo=(-(1:9)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(quantmod)
library(TTR)
oh_lc <- rutils::etf_env$VTI
VTI_close <- quantmod::Cl(oh_lc)
VTI_vol <- quantmod::Vo(oh_lc)
v_wap <- TTR::VWAP(price=VTI_close, volume=VTI_vol, n=10)
VTI_close <- VTI_close["2009-02/2009-03"]
oh_lc <- oh_lc["2009-02/2009-03"]
v_wap <- v_wap["2009-02/2009-03"]
# Plot OHLC candlechart with volume
chartSeries(oh_lc, name="VTI plus VWAP shaded",
            theme=chartTheme("white"))
# Add VWAP to main plot
addTA(ta=v_wap, on=1, col='red')
# Add price minus VWAP in extra panel
addTA(ta=(VTI_close-v_wap), col='red')
# Add background shading of areas
addTA((VTI_close-v_wap) > 0, on=-1,
      col="lightgreen", border="lightgreen")
addTA((VTI_adj-v_wap) < 0, on=-1,
      col="lightgrey", border="lightgrey")
# Add vertical and horizontal lines at v_wap minimum
addLines(v=which.min(v_wap), col='red')
addLines(h=min(v_wap), col='red')
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/chartSeries_addTA_shade-7}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Time Series Using \texttt{chart\_Series()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart\_Series()} from package \emph{quantmod} is an improved version of \texttt{chartSeries()}, with better aesthetics.
      \vskip1ex
      \texttt{chart\_Series()} plots are compatible with the base \texttt{graphics} package in \texttt{R}, so that standard plotting functions can be used in conjunction with \texttt{chart\_Series()}.
      <<chart_Series_shaded,echo=(-(1:9)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(quantmod)
library(TTR)
oh_lc <- rutils::etf_env$VTI
VTI_adj <- quantmod::Cl(oh_lc)
VTI_vol <- quantmod::Vo(oh_lc)
v_wap <- TTR::VWAP(price=VTI_adj, volume=VTI_vol, n=10)
VTI_adj <- VTI_adj["2009-02/2009-03"]
oh_lc <- oh_lc["2009-02/2009-03"]
v_wap <- v_wap["2009-02/2009-03"]
# OHLC candlechart VWAP in main plot,
chart_Series(x=oh_lc, # volume in extra panel
             TA="add_Vo(); add_TA(v_wap, on=1)",
             name="VTI plus VWAP shaded")
# Add price minus VWAP in extra panel
add_TA(VTI_adj-v_wap, col='red')
# Add background shading of areas
add_TA((VTI_adj-v_wap) > 0, on=-1,
      col="lightgreen", border="lightgreen")
add_TA((VTI_adj-v_wap) < 0, on=-1,
      col="lightgrey", border="lightgrey")
# Add vertical and horizontal lines
abline(v=which.min(v_wap), col='red')
abline(h=min(v_wap), col='red')
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/chart_Series_shaded}\\
      \texttt{chart\_Series()} also has its own functions for adding indicators: \texttt{add\_TA()}, \texttt{add\_BBands()}, etc.
      \vskip1ex
      Note that functions associated with \texttt{chart\_Series()} contain an underscore in their name,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plot and Theme Objects of \texttt{chart\_Series()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart\_Series()} creates a \emph{plot object} and returns it \emph{invisibly}.
      \vskip1ex
      A \emph{plot object} is an environment of class \emph{replot}, containing parameters specifying a plot.
      \vskip1ex
      A plot can be rendered by calling, plotting, or printing the \emph{plot object}.
      \vskip1ex
      A plot \emph{theme object} is a \texttt{list} containing parameters that determine the plot appearance (colors, size, fonts).
      \vskip1ex
      The function \texttt{chart\_theme()} returns the \emph{theme object}.
      \vskip1ex
      \texttt{chart\_Series()} plots can be modified by modifying \emph{plot objects} or \emph{theme objects}.
      \vskip1ex
      Plot and theme objects can be modified directly, or by using accessor and setter functions.
      \vskip1ex
      The parameter \texttt{"plot=FALSE"} suppresses plotting and allows modifying \emph{plot objects}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
library(quantmod)
oh_lc <- rutils::etf_env$VTI["2009-02/2009-03"]
# Extract plot object
ch_ob <- chart_Series(x=oh_lc, plot=FALSE)
class(ch_ob)
ls(ch_ob)
class(ch_ob$get_ylim)
class(ch_ob$set_ylim)
# ls(ch_ob$Env)
class(ch_ob$Env$actions)
plot_theme <- chart_theme()
class(plot_theme)
ls(plot_theme)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Customizing \texttt{chart\_Series()} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{chart\_Series()} plots can be customized by modifying the plot and theme objects.
      \vskip1ex
      Plot and theme objects can be modified directly, or by using accessor and setter functions.
      \vskip1ex
      A plot is rendered by calling, plotting, or printing the plot object.
      \vskip1ex
      The parameter \texttt{"plot=FALSE"} suppresses plotting and allows modifying \emph{plot objects}.
      <<chart_Series_custom_axis,echo=(-(1:1)),eval=FALSE,fig.width=5,fig.height=4,fig.show='hide'>>=
library(quantmod)
oh_lc <- rutils::etf_env$VTI["2010-04/2010-05"]
# Extract, modify theme, format tick marks "%b %d"
plot_theme <- chart_theme()
plot_theme$format.labels <- "%b %d"
# Create plot object
ch_ob <- chart_Series(x=oh_lc,
                      theme=plot_theme, plot=FALSE)
# Extract ylim using accessor function
y_lim <- ch_ob$get_ylim()
y_lim[[2]] <- structure(
  range(quantmod::Cl(oh_lc)) + c(-1, 1),
  fixed=TRUE)
# modify plot object to reduce y-axis range
ch_ob$set_ylim(y_lim)  # use setter function
# Render the plot
plot(ch_ob)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/chart_Series_custom_axis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting \texttt{chart\_Series()} in Multiple Panels}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{chart\_Series()} plots are compatible with the base \texttt{graphics} package, allowing easy plotting in multiple panels.
      \vskip1ex
      The parameter \texttt{"plot=FALSE"} suppresses plotting and allows adding extra plot elements.
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)
# Calculate VTI and XLF volume-weighted average price
v_wap <-
  TTR::VWAP(price=quantmod::Cl(rutils::etf_env$VTI),
            volume=quantmod::Vo(rutils::etf_env$VTI), n=10)
XLF_vwap <-
  TTR::VWAP(price=quantmod::Cl(rutils::etf_env$XLF),
            volume=quantmod::Vo(rutils::etf_env$XLF), n=10)
# Open graphics device, and define
# Plot area with two horizontal panels
x11(); par(mfrow=c(2, 1))
ch_ob <- chart_Series(  # Plot in top panel
  x=etf_env$VTI["2009-02/2009-04"],
  name="VTI", plot=FALSE)
add_TA(v_wap["2009-02/2009-04"],
       lwd=2, on=1, col='blue')
ch_ob <- chart_Series(  # Plot in bottom panel
  x=etf_env$XLF["2009-02/2009-04"],
  name="XLF", plot=FALSE)
add_TA(XLF_vwap["2009-02/2009-04"],
       lwd=2, on=1, col='blue')
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/chart_Series_panels.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting \protect\emph{OHLC} Time Series Using Package \protect\emph{dygraphs}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{dygraph()} from package \emph{dygraphs} creates interactive plots for \emph{xts} time series.
      \vskip1ex
      The function \texttt{dyCandlestick()} creates a \emph{candlestick} plot object for \emph{OHLC} data, and uses the first four columns to plot \emph{candlesticks}, and it plots any additional columns as lines.
      <<echo=TRUE,eval=FALSE>>=
library(dygraphs)
# Calculate volume-weighted average price
oh_lc <- rutils::etf_env$VTI
v_wap <- TTR::VWAP(price=quantmod::Ad(oh_lc),
    volume=quantmod::Vo(oh_lc), n=20)
# Add VWAP to OHLC  data
oh_lc <- cbind(oh_lc[, c(1:3, 6)],
               v_wap)["2009-02/2009-04"]
# Create dygraphs object
dy_graph <- dygraphs::dygraph(oh_lc)
# Convert dygraphs object to candlestick plot
dy_graph <- dygraphs::dyCandlestick(dy_graph)
# Render candlestick plot
dy_graph
# Candlestick plot using pipes syntax
dygraphs::dygraph(oh_lc) %>% dyCandlestick()
# Candlestick plot without using pipes syntax
dygraphs::dyCandlestick(dygraphs::dygraph(oh_lc))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth, height=0.35\paperwidth]{figure/dygraphs_candlestick.png}\\
      Each \emph{candlestick} displays one period of data, and consists of a box representing the \emph{Open} and \emph{Close} prices, and a vertical line representing the \emph{High} and \emph{Low} prices.
      \vskip1ex
      The color of the box signifies whether the \emph{Close} price was higher or lower than the \emph{Open},
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{dygraphs} \protect\emph{OHLC} Plots With Background Shading}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{dyShading()} adds shading to a \emph{dygraphs} plot object.
      <<echo=TRUE,eval=FALSE>>=
# Create candlestick plot with background shading
in_dex <- index(oh_lc)
in_dic <-
  rutils::diff_it(oh_lc[, 4] > oh_lc[, "VWAP"])
in_dic <- rbind(cbind(which(in_dic==1), 1),
  cbind(which(in_dic==(-1)), -1))
in_dic <- in_dic[order(in_dic[, 1]), ]
in_dic <- rbind(c(1, -in_dic[1, 2]), in_dic,
  c(NROW(oh_lc), -in_dic[NROW(in_dic), 2]))
in_dic <-
  data.frame(in_dex[in_dic[, 1]], in_dic[, 2])
# Create dygraphs object
dy_graph <- dygraphs::dygraph(oh_lc) %>%
  dyCandlestick()
# Add shading
for (i in 1:(NROW(in_dic)-1)) {
  if (in_dic[i, 2] == 1)
    dy_graph <- dy_graph %>% dyShading(from=in_dic[i, 1], to=in_dic[i+1, 1], color="lightgreen")
  else
    dy_graph <- dy_graph %>% dyShading(from=in_dic[i, 1], to=in_dic[i+1, 1], color="antiquewhite")
}  # end for
# Render plot
dy_graph
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth, height=0.35\paperwidth]{figure/dygraphs_candlestick_shaded.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{dygraphs} Plots With Two \texttt{"y"} Axes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{dyAxis()} from package \emph{dygraphs} plots customized axes to a \emph{dygraphs} plot object.
      \vskip1ex
      The function \texttt{dySeries()} adds a time series to a \emph{dygraphs} plot object.
      <<echo=TRUE,eval=FALSE>>=
library(dygraphs)
# Prepare VTI and IEF prices
price_s <- cbind(Ad(rutils::etf_env$VTI),
                 Ad(rutils::etf_env$IEF))
price_s <- na.omit(price_s)
col_names <- rutils::get_name(colnames(price_s))
colnames(price_s) <- col_names
# dygraphs plot with two y-axes
library(dygraphs)
dygraphs::dygraph(price_s, main=paste(col_names, collapse=" and ")) %>%
  dyAxis(name="y", label="VTI", independentTicks=TRUE) %>%
  dyAxis(name="y2", label="IEF", independentTicks=TRUE) %>%
  dySeries(name="VTI", axis="y", strokeWidth=2, col="red") %>%
  dySeries(name="IEF", axis="y2", strokeWidth=2, col="blue")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth, height=0.35\paperwidth]{figure/dygraphs_2yaxis.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{zoo} Plots With Two \texttt{"y"} Axes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{plot.zoo()} plots time series.
      \vskip1ex
      The function \texttt{axis()} plots customized axes in an existing plot.
      <<echo=TRUE,eval=FALSE>>=
# Open plot window and set plot margins
x11(width=6, height=5)
par(mar=c(2, 2, 2, 2), oma=c(1, 1, 1, 1))
# Plot first time series without x-axis
zoo::plot.zoo(price_s[, 1], lwd=2, col="orange",
              xlab=NA, ylab=NA, xaxt="n")
# Create X-axis date labels and add X-axis
in_dex <- pretty(index(price_s))
axis(side=1, at=in_dex, labels=format(in_dex, "%b-%d-%y"))
# Plot second time series without y-axis
par(new=TRUE)  # Allow new line on same plot
zoo::plot.zoo(price_s[, 2], xlab=NA, ylab=NA,
              lwd=2, yaxt="n", col="blue", xaxt="n")
# Plot second y-axis on right
axis(side=4, lwd=2, col="blue")
# Add axis labels
mtext(col_names[1], cex=1.2, lwd=3, side=2, las=2, adj=(-0.5), padj=(-5), col="orange")
mtext(col_names[2], cex=1.2, lwd=3, side=4, las=2, adj=1.5, padj=(-5), col="blue")
# Add title and legend
title(main=paste(col_names, collapse=" and "), line=0.5)
legend("top", legend=col_names, cex=1.0, bg="white",
       lty=1, lwd=6, col=c("orange", "blue"), bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth, height=0.35\paperwidth]{figure/zoo_2yaxis.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Package \protect\emph{qmao} for Quantitative Financial Modeling}


%%%%%%%%%%%%%%%
\subsection{draft: Package \protect\emph{qmao} for Quantitative Financial Modeling}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{qmao} is designed for downloading, manipulating, and visualizing \emph{OHLC} time series data,
package \emph{quantmod}
      \vskip1ex
      \emph{qmao} uses time series of class \texttt{"xts"}, and provides many useful functions for building quantitative financial models:
      \begin{itemize}
        \item \texttt{getSymbols()} for downloading data from external sources (\emph{Yahoo}, \emph{FRED}, etc.),
        \item \texttt{getFinancials()} for downloading financial statements,
        \item \texttt{adjustOHLC()} for adjusting \emph{OHLC} data,
        \item \texttt{Op()}, \texttt{Ad()}, \texttt{Vo()}, etc. for extracting \emph{OHLC} data columns,
        \item \texttt{periodReturn()}, \texttt{dailyReturn()}, etc. for calculating periodic returns,
        \item \texttt{chartSeries()} for candlestick plots of \emph{OHLC} data,
        \item \texttt{addBBands()}, \texttt{addMA()}, \texttt{addVo()}, etc. for adding technical indicators (Moving Averages, Bollinger Bands) and volume data to a plot,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load package qmao
library(qmao)
# Get documentation for package qmao
# Get short description
packageDescription("qmao")
# Load help page
help(package="qmao")
# List all datasets in "qmao"
data(package="qmao")
# List all objects in "qmao"
ls("package:qmao")
# Remove qmao from search path
detach("package:qmao")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Time Series of Asset Prices}


%%%%%%%%%%%%%%%
\subsection{Monte Carlo Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Monte Carlo} simulation consists of generating random samples from a given probability distribution.
      \vskip1ex
      The \emph{Monte Carlo} data samples can then used to calculate different parameters of the probability distribution (moments, quantiles, etc.), and its functionals.
      \vskip1ex
      The \emph{quantile} of a probability distribution is the value of the \emph{random variable} \texttt{x}, such that the probability of values less than \texttt{x} is equal to the given \emph{probability} $p$.
      \vskip1ex
      The \emph{quantile} of a data sample can be calculated by first sorting the sample, and then finding the value corresponding closest to the given \emph{probability} $p$.
      \vskip1ex
      The function \texttt{quantile()} calculates the sample quantiles, but it's quite slow.
      \vskip1ex
      The function \texttt{sort()} returns a vector sorted into ascending order.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1000
da_ta <- rnorm(n_rows)
# Sample mean - MC estimate
mean(da_ta)
# Sample standard deviation - MC estimate
sd(da_ta)
# Monte Carlo estimate of cumulative probability
da_ta <- sort(da_ta)
pnorm(1)
sum(da_ta<1)/n_rows
# Monte Carlo estimate of quantile
conf_level <- 0.99
qnorm(conf_level)
cut_off <- conf_level*n_rows
da_ta[cut_off]
quantile(da_ta, probs=conf_level)
# Analyze the source code of quantile()
stats:::quantile.default
# Microbenchmark quantile
library(microbenchmark)
summary(microbenchmark(
  monte_carlo=da_ta[cut_off],
  quan_tile=quantile(da_ta, probs=conf_level),
  times=100))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Brownian Motion Using \texttt{while()} Loops}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{while()} loops are often used in simulations, when the number of required loops is unknown in advance.
      \vskip1ex
      Below is an example of a simulation of the path of \emph{Brownian Motion} crossing a barrier level.
      \vspace{-1em}
        <<simu_while,eval=FALSE,echo=(-(1:3)),fig.show='hide'>>=
x11(width=6, height=5)
par(oma=c(1, 1, 1, 1), mar=c(2, 2, 2, 1), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
set.seed(1121)  # Reset random number generator
bar_rier <- 20  # Barrier level
n_rows <- 1000  # Number of simulation steps
pa_th <- numeric(n_rows)  # Allocate path vector
pa_th[1] <- 0  # Initialize path
in_dex <- 2  # Initialize simulation index
while ((in_dex <= n_rows) &&
         (pa_th[in_dex - 1] < bar_rier)) {
# Simulate next step
  pa_th[in_dex] <-
    pa_th[in_dex - 1] + rnorm(1)
  in_dex <- in_dex + 1  # Advance in_dex
}  # end while
# Fill remaining pa_th after it crosses bar_rier
if (in_dex <= n_rows)
  pa_th[in_dex:n_rows] <- pa_th[in_dex - 1]
# Create daily time series starting 2011
ts_path <- ts(data=pa_th, frequency=365, start=c(2011, 1))
plot(ts_path, type="l", col="black",
     lty="solid", lwd=2, xlab="", ylab="")
abline(h=bar_rier, lwd=2, col="red")
title(main="Brownian motion crossing a barrier level",
      line=0.5)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/simu_brown_barrier.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Brownian Motion Using Vectorized Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Simulations in \texttt{R} can be accelerated by pre-computing a vector of random numbers, instead of generatng them one at a time in a loop.
      \vskip1ex
      Vectors of random numbers allow using \emph{vectorized} functions, instead of inefficient (slow) \texttt{while()} loops.
      \vspace{-1em}
        <<simu_vector,eval=FALSE,echo=(-(1:3)),fig.show='hide'>>=
x11(width=6, height=5)
par(oma=c(1, 1, 1, 1), mar=c(2, 2, 2, 1), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
set.seed(1121)  # Reset random number generator
bar_rier <- 20  # Barrier level
n_rows <- 1000  # Number of simulation steps
# Simulate path of Brownian motion
pa_th <- cumsum(rnorm(n_rows))
# Find index when pa_th crosses bar_rier
cro_ss <- which(pa_th > bar_rier)
# Fill remaining pa_th after it crosses bar_rier
if (NROW(cro_ss)>0) {
  pa_th[(cro_ss[1]+1):n_rows] <-
    pa_th[cro_ss[1]]
}  # end if
# Create daily time series starting 2011
ts_path <- ts(data=pa_th, frequency=365,
             start=c(2011, 1))
# Create plot with horizontal line
plot(ts_path, type="l", col="black",
     lty="solid", lwd=2, xlab="", ylab="")
abline(h=bar_rier, lwd=2, col="red")
title(main="Brownian motion crossing a barrier level",
      line=0.5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/simu_brown_barrier.png}
      The trade-off between speed and memory usage: more memory may be used than necessary, since the simulation may stop before all the pre-computed random numbers are used up.
      \vskip1ex
      But the simulation is much faster because the path is simulated using \emph{vectorized} functions,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Geometric Brownian Motion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the percentage asset returns $r_t \mathrm{d} t = \mathrm{d} \log{P_t}$ follow \emph{Brownian motion}:
      \begin{displaymath}
        r_t \mathrm{d} t = \mathrm{d} \log{P_t} = ( \mu - \frac{\sigma^2}{2} ) \mathrm{d}t + \sigma \, \mathrm{d} W_t
      \end{displaymath}
      Then asset prices $P_t$ follow \emph{Geometric Brownian motion} (GBM):
      \begin{displaymath}
        \mathrm{d} P_t = \mu P_t \mathrm{d}t + \sigma \, P_t \mathrm{d} W_t
      \end{displaymath}
      Where $\sigma$ is the volatility of asset returns, and $W_t$ is a \emph{Brownian motion}, with $\mathrm{d} W_t$ following the standard normal distribution $\phi(0, \sqrt{\mathrm{d}t})$.
      \vskip1ex
      The solution of \emph{Geometric Brownian motion} is equal to:
      \begin{displaymath}
        P_t = P_0 \exp[( \mu - \frac{\sigma^2}{2} ) t + \sigma \, W_t]
      \end{displaymath}
      The convexity correction: $-\frac{\sigma^2}{2}$ ensures that the growth rate of prices is equal to $\mu$, (in accordance with Ito's lemma).
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/brown_geom.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Define daily volatility and growth rate
sigma_r <- 0.01; dri_ft <- 0.0; len_gth <- 1000
# Simulate geometric Brownian motion
re_turns <- sigma_r*rnorm(len_gth) +
  dri_ft - sigma_r^2/2
price_s <- exp(cumsum(re_turns))
plot(price_s, type="l",
     xlab="periods", ylab="prices",
     main="geometric Brownian motion")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Random \protect\emph{OHLC} Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Random \emph{OHLC} prices are useful for testing financial models.
      <<echo=TRUE,eval=FALSE>>=
# Simulate geometric Brownian motion
sigma_r <- 0.01/sqrt(48)
dri_ft <- 0.0
len_gth <- 1e4
in_dex <- seq(from=as.POSIXct(paste(Sys.Date()-250, "09:30:00")),
  length.out=len_gth, by="30 min")
price_s <- xts(exp(cumsum(sigma_r*rnorm(len_gth) + dri_ft - sigma_r^2/2)),
  order.by=in_dex)
price_s <- cbind(price_s,
  volume=sample(x=10*(2:18), size=len_gth, replace=TRUE))
# Aggregate to daily OHLC data
oh_lc <- xts::to.daily(price_s)
quantmod::chart_Series(oh_lc, name="random prices")
# dygraphs candlestick plot using pipes syntax
library(dygraphs)
dygraphs::dygraph(oh_lc[, 1:4]) %>%
  dyCandlestick()
# dygraphs candlestick plot without using pipes syntax
dygraphs::dyCandlestick(dygraphs::dygraph(oh_lc[, 1:4]))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/random_ohlc.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Log-normal} Probability Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If \texttt{x} follows the \emph{Normal} distribution $\phi(x, \mu, \sigma)$, then the exponential of \texttt{x}: $y = e^x$ follows the \emph{Log-normal} distribution $\log\phi()$:
      \begin{displaymath}
        \log\phi(y, \mu, \sigma) = \frac{\exp(-(\log{y} - \mu)^2/2 \sigma^2)}{y \sigma \, \sqrt{2 \pi}}
      \end{displaymath}
      With mean equal to: $\bar{y} = \mathbb{E}[y] = \exp(\mu + \sigma^2/2)$, and median equal to: $\tilde{y} = \exp(\mu)$
      <<echo=TRUE,eval=FALSE>>=
# Standard deviations of log-normal distribution
sig_mas <- c(0.5, 1, 1.5)
# Create plot colors
col_ors <- c("black", "red", "blue")
# Plot all curves
for (in_dex in 1:NROW(sig_mas)) {
  curve(expr=dlnorm(x, sdlog=sig_mas[in_dex]),
        type="l", xlim=c(0, 3), lwd=2,
        xlab="", ylab="", col=col_ors[in_dex],
        add=as.logical(in_dex-1))
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/log_norm_dist.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Add title and legend
title(main="Log-normal Distributions", line=0.5)
legend("topright", inset=0.05, title="Sigmas",
       paste("sigma", sig_mas, sep="="),
       cex=0.8, lwd=2, lty=rep(1, NROW(sig_mas)),
       col=col_ors)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Standard Deviation of \protect\emph{Log-normal} Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vskip1ex
      If percentage asset returns are \emph{normally} distributed and follow \emph{Brownian motion}, then asset prices follow \emph{Geometric Brownian motion}, and they are \emph{Log-normally} distributed at every point in time.
      \vskip1ex
      The standard deviation of \emph{log-normal} prices is equal to the return volatility $\sigma_r$ times the square root of time: $\sigma = \sigma_r \sqrt{t}$.
      \vskip1ex
      The \emph{Log-normal} distribution has a strong positive skewness (third moment) equal to: $\varsigma = \mathbb{E}[(y - \mathbb{E}[y])^3] = (e^{\sigma^2} + 2) \sqrt{e^{\sigma^2} - 1}$
      \vskip1ex
      For large standard deviation, the skewness increases exponentially with the standard deviation and with time: $\varsigma \propto e^{1.5 \sigma^2} = e^{1.5 t \sigma^2_r}$
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(4, 4, 3, 1))
# Return volatility of VTI ETF
sigma_r <- sd(rutils::diff_it(log(rutils::etf_env$VTI[, 4])))
sigmar_2 <- sigma_r^2
n_rows <- NROW(rutils::etf_env$VTI)
# Standard deviation of log-normal prices
sqrt(n_rows)*sigma_r
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/log_norm_skew.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Skewness of log-normal prices
skew_ness <- function(t) {
  ex_p <- exp(t*sigmar_2)
  (ex_p + 2)*sqrt(ex_p - 1)
}  # end skew_ness
curve(expr=skew_ness, xlim=c(1, n_rows), lwd=3,
      xlab="Number of days", ylab="Skewness", col="blue",
      main="Skewness of Log-normal Prices
      as a Function of Time")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Mean and Median of \protect\emph{Log-normal} Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The mean of the \emph{Log-normal} distribution: $\bar{y} = \mathbb{E}[y] = \exp(\mu + \sigma^2/2)$ is greater than its median, which is equal to: $\tilde{y} = \exp(\mu)$.
      \vskip1ex
      So if stock prices follow \emph{Geometric Brownian motion} and are distributed \emph{log-normally}, then a stock selected at random will have a high probability of havng a lower price than the mean expected price.
      \vskip1ex
      The cumulative \emph{Log-normal} probability distribution is equal to $\operatorname{F}(x) = \Phi(\frac{\log{y}-\mu}{\sigma})$, where $\Phi()$ is the cumulative standard normal distribution.
      \vskip1ex
      So the probability that the price of a randomly selected stock will be lower than the mean price is equal to $\operatorname{F}(\bar{y}) = \Phi(\sigma/2)$.
      \vskip1ex
      Therefore an investor without skill, who selects stocks at random, has a high probability of underperforming the index.
      \vskip1ex
      Performing as well as the index requires \emph{significant} investment skill, while outperforming the index requires \emph{exceptional} investment skill.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/log_norm_prob.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Probability that random log-normal price will be lower than the mean price
curve(expr=pnorm(sigma_r*sqrt(x)/2),
      xlim=c(1, n_rows), lwd=3,
      xlab="Number of days", ylab="Probability", col="blue",
      main="Probability That Random Log-normal Price
      Will be Lower Than the Mean Price")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Paths of Geometric Brownian Motion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard deviation of \emph{log-normal} prices is equal to the return volatility $\sigma_r$ times the square root of time: $\sigma = \sigma_r \sqrt{t}$.
      \vskip1ex
      For large standard deviation, the skewness $\varsigma$ increases exponentially with the standard deviation and with time: $\varsigma \propto e^{1.5 \sigma^2} = e^{1.5 t \sigma^2_r}$
      <<echo=TRUE,eval=FALSE>>=
# Define daily volatility and growth rate
sigma_r <- 0.01; dri_ft <- 0.0; len_gth <- 5000
path_s <- 10
# Simulate multiple paths of geometric Brownian motion
price_s <- matrix(sigma_r*rnorm(path_s*len_gth) +
    dri_ft - sigma_r^2/2, nc=path_s)
price_s <- exp(matrixStats::colCumsums(price_s))
# Create xts time series
price_s <- xts(price_s, order.by=seq.Date(Sys.Date()-NROW(price_s)+1, Sys.Date(), by=1))
# Plot xts time series
col_ors <- colorRampPalette(c("red", "blue"))(NCOL(price_s))
col_ors <- col_ors[order(order(price_s[NROW(price_s), ]))]
par(mar=c(3, 3, 2, 2), oma=c(0, 0, 0, 0))
plot.zoo(price_s, main="Multiple paths of geometric Brownian motion",
         xlab=NA, ylab=NA, plot.type="single", col=col_ors)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/brown_geom_paths.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Paths of Geometric Brownian Motion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Prices following \emph{Geometric Brownian motion} have a large positive skewness, so that the expected value of prices is skewed by a few paths with very high prices, while the prices of the majority of paths are below their expected value.
      \vskip1ex
      For large standard deviation, the skewness $\varsigma$ increases exponentially with the standard deviation and with time: $\varsigma \propto e^{1.5 \sigma^2} = e^{1.5 t \sigma^2_r}$
      <<echo=TRUE,eval=FALSE>>=
# Define daily volatility and growth rate
sigma_r <- 0.01; dri_ft <- 0.0; len_gth <- 10000
path_s <- 100
# Simulate multiple paths of geometric Brownian motion
price_s <- matrix(sigma_r*rnorm(path_s*len_gth) +
    dri_ft - sigma_r^2/2, nc=path_s)
price_s <- exp(matrixStats::colCumsums(price_s))
# Calculate percentage of paths below the expected value
per_centage <- rowSums(price_s < 1.0) / path_s
# Create xts time series of percentage of paths below the expected value
per_centage <- xts(per_centage, order.by=seq.Date(Sys.Date()-NROW(per_centage)+1, Sys.Date(), by=1))
# Plot xts time series of percentage of paths below the expected value
par(mar=c(3, 3, 2, 2), oma=c(0, 0, 0, 0))
plot.zoo(per_centage, main="Percentage of GBM paths below mean",
         xlab=NA, ylab=NA, col="blue")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/brown_geom_percent.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Time Evolution of Stock Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Stock prices evolve in time similarly to \emph{Geometric Brownian motion}, and they also exhibit a very skewed distribution of prices.
      <<echo=TRUE,eval=FALSE>>=
# Load S&P500 stock prices
load("C:/Develop/lecture_slides/data/sp500.RData")
ls(sp500_env)
# Extract closing prices
price_s <- eapply(sp500_env, quantmod::Cl)
# Flatten price_s into a single xts series
price_s <- rutils::do_call(cbind, price_s)
# Carry forward and backward non-NA prices
price_s <- xts:::na.locf.xts(price_s)
price_s <- xts:::na.locf.xts(price_s, fromLast=TRUE)
sum(is.na(price_s))
# Rename and normalize columns
colnames(price_s) <- sapply(colnames(price_s),
  function(col_name) strsplit(col_name, split="[.]")[[1]][1])
price_s <- xts(t(t(price_s) / as.numeric(price_s[1, ])),
               order.by=index(price_s))
# Calculate permution index for sorting the lowest to highest final price_s
or_der <- order(price_s[NROW(price_s), ])
# Select a few symbols
sym_bols <- colnames(price_s)[or_der]
sym_bols <- sym_bols[seq.int(from=1, to=(NROW(sym_bols)-1), length.out=20)]
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/stock_index_paths.png}
    \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot xts time series of price_s
col_ors <- colorRampPalette(c("red", "blue"))(NROW(sym_bols))
col_ors <- col_ors[order(order(price_s[NROW(price_s), sym_bols]))]
par(mar=c(3, 3, 2, 2), oma=c(0, 0, 0, 0))
plot.zoo(price_s[, sym_bols], main="20 S&P500 stock prices (normalized)",
         xlab=NA, ylab=NA, plot.type="single", col=col_ors)
legend(x="topleft", inset=0.05, cex=0.8,
       legend=rev(sym_bols), col=rev(col_ors), lwd=6, lty=1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Stock Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Usually, a small number of stocks in an index reach very high prices, while the prices of the majority of stocks remain below the index price (the average price of the index portfolio).
      \vskip1ex
      For example, the current prices of almost \texttt{80\%} of the S\&P500 constituent stocks from \texttt{1990} are now below the average price of that portfolio.
      \vskip1ex
      Therefore an investor without skill, who selects stocks at random, has a high probability of underperforming the index, because they will most likely miss selecting the best performing stocks.
      \vskip1ex
      Performing as well as the index requires \emph{significant} investment skill, while outperforming the index requires \emph{exceptional} investment skill.
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate average of valid stock prices
val_id <- (price_s != 1)  # valid stocks
n_stocks <- rowSums(val_id)
n_stocks[1] <- NCOL(price_s)
in_dex <- rowSums(price_s * val_id) / n_stocks
# Calculate percentage of stock prices below the average price
per_centage <- rowSums((price_s < in_dex) & val_id) / n_stocks
# Create xts time series of average stock prices
in_dex <- xts(in_dex, order.by=index(price_s))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
    %   \includegraphics[width=0.5\paperwidth]{figure/stock_index_prices.png}
    % \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/stock_index_prices_percent.png}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=4)
par(mar=c(3, 3, 2, 2), oma=c(0, 0, 0, 0))
# Plot xts time series of average stock prices
plot.zoo(in_dex, main="Average S&P500 stock prices (normalized from 1990)",
         xlab=NA, ylab=NA, col="blue")
# Create xts time series of percentage of stock prices below the average price
per_centage <- xts(per_centage, order.by=index(price_s))
# Plot percentage of stock prices below the average price
plot.zoo(per_centage[-(1:2),],
         main="Percentage of S&P500 stock prices below the average price",
         xlab=NA, ylab=NA, col="blue")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Time Series Modeling}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation Function of Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Autocorrelation Function} (\emph{ACF}) consists of the autocorrelation coefficients of a time series:
      \begin{displaymath}
        \rho_k = \frac{\sum_{i=k+1}^n (x_i-\bar{x})(x_{i-k}-\bar{x})}{(n-k) \, \sigma^2}
      \end{displaymath}
      \vskip1ex
      The function \texttt{stats::acf()} calculates and plots the autocorrelation function of a time series.
      \vskip1ex
      The function \texttt{stats::acf()} has the drawback that it plots the lag zero autocorrelation (which is simply equal to $1$).
      \vskip1ex
      The function \texttt{as.numeric()} coerces complex data objects into \texttt{numeric} vectors, and removes all their \emph{attributes}.
        <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=4)
par(mar=c(3, 3, 1, 3), oma=c(0, 0, 0, 0), mgp=c(2, 0.5, 0))
re_turns <- 
  diff(log(as.numeric(EuStockMarkets[, 1])))
# Plot autocorrelations using stats::acf()
stats::acf(re_turns, lag=10, main="")
title(main="ACF of DAX Returns", line=-1)
# Two-tailed 95% confidence interval
qnorm(0.975)/sqrt(NROW(re_turns))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/acf_dax.png}\\
      The \emph{DAX} time series of returns does not appear to have statistically significant autocorrelations.
      \vskip1ex
      The horizontal dashed lines are two-tailed confidence intervals of the autocorrelation estimator at \texttt{95\%} significance level: $\frac{\Phi^{-1}(0.975)}{\sqrt{n}}$.
      \vskip1ex
      But the visual inspection of the \emph{ACF} plot alone is not enough to test whether autocorrelations are statistically significant or not.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Ljung-Box Test for Autocorrelations of Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Ljung-Box} test, tests if the autocorrelations of a time series are \emph{statistically significant}.
      \vskip1ex
      The \emph{null hypothesis} of the \emph{Ljung-Box} test is that the autocorrelations are equal to zero.
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        Q = n(n+2) \sum_{k=1}^{maxlag} \frac{{\hat\rho}_k^2}{n-k}
      \end{displaymath}
      Where \texttt{n} is the sample size, and the ${\hat\rho}_k$ are sample autocorrelations.
      \vskip1ex
      The \emph{Ljung-Box} statistic follows the \emph{chi-squared} distribution with \emph{maxlag} degrees of freedom.
      \vskip1ex
      The \emph{Ljung-Box} statistic is small for time series that have \emph{statistically insignificant} autocorrelations.
      \vskip1ex
      The function \texttt{Box.test()} calculates the \emph{Ljung-Box} test and returns the test statistic and its p-value.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Ljung-Box test for DAX returns
# 'lag' is the number of autocorrelation coefficients
Box.test(re_turns, lag=10, type="Ljung")
library(Ecdat)  # Load Ecdat
macro_zoo <- as.zoo(Macrodat[, c("lhur", "fygm3")])
colnames(macro_zoo) <- c("unemprate", "3mTbill")
macro_diff <- na.omit(diff(macro_zoo))
# Changes in 3 month T-bill rate are autocorrelated
Box.test(macro_diff[, "3mTbill"],
         lag=10, type="Ljung")
# Changes in unemployment rate are autocorrelated
Box.test(macro_diff[, "unemprate"],
         lag=10, type="Ljung")
      @
      The \emph{p}-value for \emph{DAX} returns is large, and we conclude that the \emph{null hypothesis} is \texttt{TRUE}, and that \emph{DAX} returns are \emph{not} autocorrelated.
      \vskip1ex
      The \emph{p}-value for changes in econometric data is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and that econometric data \emph{are} autocorrelated.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Improved Autocorrelation Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.45\textwidth}
      The function \texttt{acf()} has the drawback that it plots the lag zero autocorrelation (which is simply equal to $1$).
      \vskip1ex
      Inspection of the data returned by \texttt{acf()} shows how to omit the lag zero autocorrelation.
      \vskip1ex
      The function \texttt{acf()} returns the \emph{ACF} data invisibly, i.e. the return value can be assigned to a variable, but otherwise it isn't automatically printed to the console.
      \vskip1ex
      The function \texttt{rutils::plot\_acf()} from package \emph{rutils} is a wrapper for \texttt{acf()}, and it omits the lag zero autocorrelation.
      <<echo=TRUE,eval=FALSE>>=
# Get the ACF data returned invisibly
acf_data <- acf(re_turns, plot=FALSE)
summary(acf_data)
# Print the ACF data
print(acf_data)
dim(acf_data$acf)
dim(acf_data$lag)
head(acf_data$acf)
      @
    \column{0.55\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
plot_acf <- function(x_ts, lagg=10,
                     plo_t=TRUE,
                     xlab="Lag", ylab="",
                     main="", ...) {
  # Calculate the ACF without a plot
  acf_data <- acf(x=x_ts, lag.max=lagg, plot=FALSE, ...)
  # Remove first element of ACF data
  acf_data$acf <- array(data=acf_data$acf[-1],
    dim=c((dim(acf_data$acf)[1]-1), 1, 1))
  acf_data$lag <- array(data=acf_data$lag[-1],
    dim=c((dim(acf_data$lag)[1]-1), 1, 1))
  # Plot ACF
  if (plo_t) {
    ci <- qnorm((1+0.95)/2)*sqrt(1/NROW(x_ts))
    ylim <- c(min(-ci, range(acf_data$acf[-1])),
              max(ci, range(acf_data$acf[-1])))
    plot(acf_data, xlab=xlab, ylab=ylab,
         ylim=ylim, main="", ci=0)
    title(main=main, line=0.5)
    abline(h=c(-ci, ci), col="blue", lty=2)
  }
  # Return the ACF data invisibly
  invisible(acf_data)
}  # end plot_acf
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of \protect\emph{DAX} Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{DAX} time series of returns does not appear to have statistically significant autocorrelations.
      \vskip1ex
      But the visual inspection of the \emph{ACF} plot alone is not enough to test whether autocorrelations are statistically significant or not.
        <<echo=(-(1:2)),eval=FALSE>>=
library(rutils)  # Load package rutils
par(mar=c(5,0,1,2), oma=c(1,2,1,0), mgp=c(2,1,0), cex.lab=0.8, cex.axis=1.0, cex.main=0.8, cex.sub=0.5)
# Improved autocorrelation function
rutils::plot_acf(re_turns, lag=10, main="")
title(main="acf of DAX returns", line=-1)
# Ljung-Box test for DAX returns
Box.test(re_turns, lag=10, type="Ljung")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/dax_acf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Squared \protect\emph{DAX} Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Squared random returns are not autocorrelated.
      \vskip1ex
      But squared \emph{DAX} returns do have statistically significant autocorrelations.
      \vskip1ex
      The autocorrelations of squared asset returns are a very important feature.
      <<dax_squared_acf,echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=7)
par(mar=c(1, 1, 1, 1), oma=c(1, 1, 0, 1), mgp=c(0, 0.5, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # Set plot panels
# Autocorrelation of squared random returns
rutils::plot_acf(rnorm(NROW(re_turns))^2,
         lag=10, main="")
title(main="ACF of Squared Random Returns",
      line=-1)
# Autocorrelation of squared DAX returns
rutils::plot_acf(re_turns^2, lag=10, main="")
title(main="ACF of Squared DAX Returns",
      line=-1)
# Ljung-Box test for squared DAX returns
Box.test(re_turns^2, lag=10, type="Ljung")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/dax_squared_acf.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{U.S. Macroeconomic Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{Ecdat} contains the \texttt{Macrodat} U.S. macroeconomic data.
      \vskip1ex
      \texttt{"lhur"} is the unemployment rate (average of months in quarter).
      \vskip1ex
      \texttt{"fygm3"} 3 month treasury bill interest rate (last month in quarter)
      <<macro_data,echo=(-(1:1)),eval=FALSE,fig.show='hide'>>=
library(rutils)  # Load package rutils
library(Ecdat)  # Load Ecdat
colnames(Macrodat)  # United States Macroeconomic Time Series
macro_zoo <- as.zoo(  # Coerce to "zoo"
          Macrodat[, c("lhur", "fygm3")])
colnames(macro_zoo) <- c("unemprate", "3mTbill")
# ggplot2 in multiple panes
autoplot(  # Generic ggplot2 for "zoo"
  object=macro_zoo, main="US Macro",
  facets=Series ~ .) + # end autoplot
  xlab("") +
theme(  # modify plot theme
  legend.position=c(0.1, 0.5),
  plot.title=element_text(vjust=-2.0),
  plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
  plot.background=element_blank(),
  axis.text.y=element_blank()
)  # end theme
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/macro_data-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Econometric Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most econometric data displays a high degree of autocorrelation.
      \vskip1ex
      But the time series of asset returns display very low autocorrelations.
      <<macro_corr,echo=(-(1:2)),eval=FALSE,fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # Set plot panels
macro_diff <- na.omit(diff(macro_zoo))
rutils::plot_acf(coredata(macro_diff[, "unemprate"]),
  lag=10, main="quarterly unemployment rate")
rutils::plot_acf(coredata(macro_diff[, "3mTbill"]),
  lag=10, main="3 month T-bill EOQ")
      @
      The function \texttt{zoo::coredata()} extracts the underlying numeric data from a complex data object.
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/macro_corr-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Filtering Time Series Using Function \texttt{filter()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} applies a linear filter to time series, vectors, and matrices, and returns a time series of class \texttt{"ts"}.
      \vskip1ex
      The function \texttt{filter()} with the argument \texttt{method="convolution"} calculates the \emph{convolution} of the vector $r_i$ with the filter $\varphi_i$:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
      Where $f_i$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Extract time series of DAX prices
da_x <- EuStockMarkets[, 1]
# Apply convolution filter over past values (sides=1)
co_eff <- c(0.1, 0.39, 0.5)
filter_ed <- filter(da_x, filter=co_eff,
                   method="convolution", sides=1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fast Filtering Using Compiled Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} is very fast because it calculates the filter by calling compiled \texttt{C++} functions.
      \vskip1ex
      \texttt{filter()} with \texttt{method="convolution"} calls the function \texttt{stats:::C\_cfilter} to calculate the \emph{convolution}. 
      \vskip1ex
      \texttt{filter()} with \texttt{method="recursive"} calculates a \emph{recursive} filter over the vector of random \emph{innovations} $\xi_i$ as follows:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Where $r_i$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
      \vskip1ex
      The \emph{recursive} filter describes an \emph{AR(p)} process, which is a special case of an \emph{ARIMA} process.
      \vskip1ex
      \texttt{filter()} with \texttt{method="recursive"} calls the function \texttt{stats:::C\_rfilter} to calculate the \emph{recursive filter}. 
      \vskip1ex
      Filtering can be performed even faster by directly calling the compiled \texttt{C++} functions.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Inspect the R code of the function filter()
filter
# Get information about C_cfilter()
getAnywhere(C_cfilter)
# Filter over past values (sides=1)
filter_ed <- filter(da_x, filter=co_eff,
                   method="convolution", sides=1)
# Filter using C_cfilter() compiled C++ function directly
filter_fast <- .Call(stats:::C_cfilter, da_x, filter=co_eff, sides=1, circular=FALSE)
all.equal(as.numeric(filter_ed), filter_fast, check.attributes=FALSE)
# Benchmark speed of the two methods
library(microbenchmark)
summary(microbenchmark(
  fast_filter=.Call(stats:::C_cfilter, da_x, filter=co_eff, sides=1, circular=FALSE),
  R_filter=filter(da_x, filter=co_eff, method="convolution", sides=1)
  ), times=10)[, c(1, 4, 5)]
# Simulate ARIMA using filter()
in_nov <- rnorm(NROW(EuStockMarkets))
ari_ma <- filter(x=in_nov,
  filter=co_eff, method="recursive")
# Get information about C_rfilter()
getAnywhere(C_rfilter)
# Filter using C_rfilter() compiled C++ function directly
arima_fast <- .Call(stats:::C_rfilter, in_nov, co_eff, double(NROW(co_eff) + NROW(in_nov)))
all.equal(as.numeric(ari_ma), arima_fast[-(1:3)], check.attributes=FALSE)
# Benchmark speed of the two methods
summary(microbenchmark(
  fast_filter=.Call(stats:::C_rfilter, in_nov, co_eff, double(NROW(co_eff) + NROW(in_nov))),
  R_filter=filter(x=in_nov, filter=co_eff, method="recursive")
  ), times=10)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Filtered Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<dax_filter,eval=FALSE,fig.width=6,fig.height=5,fig.show='hide'>>=
library(rutils)  # Load package rutils
library(ggplot2)  # Load ggplot2
library(gridExtra)  # Load gridExtra
# Coerce to zoo and merge the time series
filter_ed <- cbind(as.zoo(da_x),
                  as.zoo(filter_ed))
colnames(filter_ed) <- c("DAX", "DAX filtered")
autoplot(  # Plot ggplot2
    window(filter_ed, start=1997, end=1998), 
    main="Filtered DAX", facets=NULL) +  # end autoplot
xlab("") + ylab("") +
theme(  # modify plot theme
    legend.position=c(0.1, 0.5),
    plot.title=element_text(vjust=-2.0),
    plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
    plot.background=element_blank(),
    axis.text.y=element_blank()
    )  # end theme
# end ggplot2
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/dax_filter-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation Function of Filtered Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filtering a time series creates autocorrelations.
      <<echo=TRUE,eval=FALSE>>=
# Open plot window
x11(width=6, height=7)
# Set plot parameters
par(oma=c(1, 1, 0, 1), mar=c(1, 1, 1, 1), mgp=c(0, 0.5, 0), 
    cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# Set two plot panels
par(mfrow=c(2,1))
# Calculate DAX returns
re_turns <- na.omit(diff(log(filter_ed)))
# Plot ACF of DAX returns
rutils::plot_acf(re_turns[, 1], 
  lag=10, xlab="")
title(main="ACF of DAX filtered Returns", line=-1)
# Plot ACF of DAX returns
rutils::plot_acf(re_turns[, 2], 
  lag=10, xlab="")
title(main="ACF of DAX Filtered", line=-1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/acf_dax_filter.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autoregressive \protect\emph{ARIMA} Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)} of order \emph{p} for a time series $r_i$ is defined as:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Where $\varphi_i$ are the \emph{AR} coefficients, and $\xi_i$ are random normal \emph{innovations} with zero mean and unit variance.
      \vskip1ex
      The \emph{AR(p)} process is a special case of an \emph{ARIMA} process, and is simply called an \emph{AR} process.
      \vskip1ex
      If the \emph{AR(p)} process is stationary then the time series $r_i$ is mean reverting to zero.
      \vskip1ex
      The function \texttt{arima.sim()} simulates \emph{ARIMA} processes, with the \texttt{"model"} argument accepting a \texttt{list} of \emph{AR} coefficients $\varphi_i$.
    <<echo=(-(1:2)),eval=FALSE>>=
# Simulate AR processes
set.seed(1121)  # Reset random numbers
in_dex <- Sys.Date() + 0:728  # two year daily series
ari_ma <- xts(  # AR time series of returns
  x=arima.sim(n=NROW(in_dex), model=list(ar=0.2)),
  order.by=in_dex)
ari_ma <- cbind(ari_ma, cumsum(ari_ma))
colnames(ari_ma) <- c("AR returns", "AR prices")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      % \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ar_process.png}
      \vspace{-3em}
    <<echo=(-(1:2)),eval=FALSE>>=
library(ggplot2)  # Load ggplot2
library(gridExtra)  # Load gridExtra
autoplot(object=ari_ma, # ggplot AR process
     facets="Series ~ .",
     main="Autoregressive process (phi=0.2)") +
  facet_grid("Series ~ .", scales="free_y") +
  xlab("") + ylab("") +
theme(legend.position=c(0.1, 0.5),
  plot.background=element_blank(),
  axis.text.y=element_blank())
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Examples of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The speed of mean reversion of an \emph{AR(1)} process depends on the \emph{AR} coefficient $\varphi_1$, with a negative coefficient producing faster mean reversion, and a positive coefficient producing stronger diversion.
      \vskip1ex
      A positive coefficient $\varphi_1$ produces a diversion away from the mean, so that the time series $r_i$ wanders away from the mean for longer periods of time.
      <<echo=TRUE,eval=FALSE>>=
ar_coeff <- c(-0.9, 0.01, 0.9)  # AR coefficients
# Create three AR time series
ari_ma <- sapply(ar_coeff, function(phi) {
  set.seed(1121)  # Reset random numbers
  arima.sim(n=NROW(in_dex), model=list(ar=phi))
})  # end sapply
colnames(ari_ma) <- paste("autocorr", ar_coeff)
plot.zoo(ari_ma, main="AR(1) prices", xlab=NA)
# Or plot using ggplot
ari_ma <- xts(x=ari_ma, order.by=in_dex)
library(ggplot)
autoplot(ari_ma, main="AR(1) prices",
         facets=Series ~ .) +
    facet_grid(Series ~ ., scales="free_y") +
xlab("") +
theme(
  legend.position=c(0.1, 0.5),
  plot.title=element_text(vjust=-2.0),
  plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
  plot.background=element_blank(),
  axis.text.y=element_blank())
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ar_processes.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)}:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Can be simulated by using an explicit recursive loop in \texttt{R}.
      \vskip1ex
      \emph{AR} processes can also be simulated by using the function \texttt{filter()} directly, with the argument \texttt{method="recursive"}.
      \vskip1ex
      The function \texttt{filter()} applies a linear filter to a vector, and returns a time series of class \texttt{"ts"}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define AR(3) coefficients and innovations
co_eff <- c(0.1, 0.39, 0.5)
len_gth <- 1e2
set.seed(1121); in_nov <- rnorm(len_gth)
# Simulate AR process using recursive loop in R
ari_ma <- numeric(NROW(in_nov))
ari_ma[1] <- in_nov[1]
ari_ma[2] <- co_eff[1]*ari_ma[1] + in_nov[2]
ari_ma[3] <- co_eff[1]*ari_ma[2] +
  co_eff[2]*ari_ma[1] + in_nov[3]
for (it in 4:NROW(ari_ma)) {
  ari_ma[it] <-
    ari_ma[(it-1):(it-3)] %*% co_eff + in_nov[it]
}  # End for
# Simulate AR process using filter()
arima_faster <- filter(x=in_nov,
  filter=co_eff, method="recursive")
class(arima_faster)
all.equal(ari_ma, as.numeric(arima_faster))
# Fast simulation of AR process using C_rfilter()
arima_fastest <- .Call(stats:::C_rfilter, in_nov, co_eff, double(NROW(co_eff) + NROW(in_nov)))[-(1:3)]
all.equal(ari_ma, arima_fastest)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Autoregressive Processes Using \texttt{arima.sim()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{arima.sim()} simulates \emph{ARIMA} processes by calling the function \texttt{filter()}.
      \vskip1ex
      \emph{ARIMA} processes can also be simulated by using the function \texttt{filter()} directly, with the argument \texttt{method="recursive"}.
      \vskip1ex
      Simulating stationary \emph{autoregressive} processes requires a \emph{warmup period}, to allow the process to reach its stationary state.
      \vskip1ex
      The required length of the \emph{warmup period} depends on the smallest root of the characteristic equation, with a longer \emph{warmup period} needed for smaller roots, that are closer to $1$.
      \vskip1ex
      The \emph{rule of thumb} (heuristic rule, guideline) is for the \emph{warmup period} to be equal to \texttt{6} divided  by the logarithm of the smallest characteristic root plus the number of \emph{AR} coefficients: $\frac{6}{\log(minroot)} + \operatorname{numcoeff}$
    \column{0.5\textwidth}
      \vspace{-1em}
    <<echo=TRUE,eval=FALSE>>=
# Calculate modulus of roots of characteristic equation
root_s <- Mod(polyroot(c(1, -co_eff)))
# Calculate warmup period
warm_up <- NROW(co_eff) + ceiling(6/log(min(root_s)))
set.seed(1121)
len_gth <- 1e4
in_nov <- rnorm(len_gth + warm_up)
# Simulate ARIMA using arima.sim()
ari_ma <- arima.sim(n=len_gth,
  model=list(ar=co_eff),
  start.innov=in_nov[1:warm_up],
  innov=in_nov[(warm_up+1):NROW(in_nov)])
# Simulate AR process using filter()
arima_fast <- filter(x=in_nov,
  filter=co_eff, method="recursive")
all.equal(arima_fast[-(1:warm_up)],
  as.numeric(ari_ma))
# Benchmark the speed of the three methods of simulating ARIMA
library(microbenchmark)
summary(microbenchmark(
  filter=filter(x=in_nov, filter=co_eff, method="recursive"),
  arima_sim=arima.sim(n=len_gth,
                      model=list(ar=co_eff),
                      start.innov=in_nov[1:warm_up],
                      innov=in_nov[(warm_up+1):NROW(in_nov)]),
  arima_loop={for (it in 4:NROW(ari_ma)) {
  ari_ma[it] <- ari_ma[(it-1):(it-3)] %*% co_eff + in_nov[it]}}
  ), times=10)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Stationary Processes and Unit-root Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A process is \emph{stationary} if its probability distribution does not change with time, which means that it has constant mean and variance.
      \vskip1ex
      The \emph{autoregressive} process \emph{AR(p)}:
      $r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i$
      \vskip1ex
      Has the following characteristic equation:
      $1 - \varphi_1 z - \varphi_2 z^2 - \ldots - \varphi_p z^p = 0$
      \vskip1ex
      An autoregressive process is stationary only if the absolute values of all the roots of its characteristic equation are greater than $1$.
      \vskip1ex
      If the sum of the autoregressive coefficients is equal to $1$: $\sum_{i=1}^p \varphi_i = 1$, then the process has a root equal to $1$ (it has a \emph{unit root}), so it's not stationary.
      \vskip1ex
      Non-stationary processes with unit roots are called \emph{unit-root} processes.
      \vskip1ex
      A simple example of a \emph{unit-root} process is the process:
      $p_i = p_{i-1} + \xi_i$,
      which is called a \emph{Wiener} process (Brownian motion, random walk).
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/stat_unit_root-1}
      \vspace{-4em}
      <<echo=(-(1:3)),eval=FALSE>>=
library(rutils)  # Load rutils
library(ggplot2)  # Load ggplot2
set.seed(1121)  # Initialize random number generator
rand_walk <- cumsum(zoo(matrix(rnorm(3*100), ncol=3),
                  order.by=(Sys.Date()+0:99)))
colnames(rand_walk) <-
  paste("rand_walk", 1:3, sep="_")
plot.zoo(rand_walk, main="Random walks",
     xlab="", ylab="", plot.type="single",
     col=c("black", "red", "blue"))
# Add legend
legend(x="topleft",
       legend=colnames(rand_walk),
       col=c("black", "red", "blue"), lty=1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Integrated and Unit-root Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The cumulative sum of a given process is called its \emph{integrated} process.
      \vskip1ex
      For example, asset prices follow an \emph{integrated} process with respect to asset returns: $p_n = {\sum_{i=1}^n r_i}$.
      \vskip1ex
      If returns follow an \emph{AR(p)} process:
      $r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i$
      \vskip1ex
      Then asset prices follow the process:
      $p_i = (1 + \varphi_1) p_{i-1} + (\varphi_2 - \varphi_1) p_{i-2} + \ldots + (\varphi_p - \varphi_{p-1}) p_{i-p} - \varphi_p p_{i-p-1} + \xi_i$
      \vskip1ex
      The sum of the coefficients of the price process is equal to $1$, so it has a \emph{unit root} for all values of the $\varphi_i$ coefficients.
      \vskip1ex
      The \emph{integrated} process of an \emph{AR(p)} process is always a \emph{unit root} process.
    \column{0.5\textwidth}
      In particular, if returns follow an \emph{AR(1)} process: $r_i = \varphi r_{i-1} + \xi_i$. 
      \vskip1ex
      Then asset prices follow the process: $p_i = (1 + \varphi) p_{i-1} - \varphi p_{i-2} + \xi_i$
      \vskip1ex
      Which is a \emph{unit root} process for all values of $\varphi$, because the sum of its coefficients is equal to $1$.
      \vskip1ex
      If $\varphi = 0$ then the above process is a \emph{Wiener} process (random walk).
      <<echo=TRUE,eval=FALSE>>=
# Simulate arima with large AR coefficient
set.seed(1121)
ari_ma <- arima.sim(n=len_gth, model=list(ar=0.99))
tseries::adf.test(ari_ma)
# Integrated series has unit root
tseries::adf.test(cumsum(ari_ma))
# Simulate arima with negative AR coefficient
set.seed(1121)
ari_ma <- arima.sim(n=len_gth, model=list(ar=-0.99))
tseries::adf.test(ari_ma)
# Integrated series has unit root
tseries::adf.test(cumsum(ari_ma))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Dickey-Fuller Test for Unit Roots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Dickey-Fuller} and \emph{Augmented Dickey-Fuller} tests are designed to test the \emph{null hypothesis} that a time series process has a \emph{unit root}.
      \vskip1ex
      The \emph{Augmented Dickey-Fuller} (\emph{ADF}) test fits the following regression model, designed to determine if the time series $p_i$ exhibits mean reversion:
      \begin{displaymath}
        r_i = \gamma p_{i-1} + \varphi_1 r_{i-1} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      where $p_i = p_{i-1} + r_i$, so that:
      \begin{displaymath}
        p_i = (1 + \gamma) p_{i-1} + \varphi_1 r_{i-1} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      If the mean reversion parameter $\gamma$ is negative: $\gamma < 0$, then the time series $p_i$ has no \emph{unit root}.
      \vskip1ex
      The \emph{null hypothesis} is that the price process has a unit root ($\gamma = 0$, no mean reversion), while the alternative hypothesis is that the price process is stationary ($\gamma < 0$, mean reversion).
      \vskip1ex
      The \emph{ADF} test statistic is equal to the \emph{t}-value of the $\gamma$ parameter: $t_{\gamma} = \hat\gamma / SE_{\gamma}$ (which follows its own distribution, different from the \texttt{t}-distribution).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
len_gth <- 1e4
# Simulate arima with small AR coefficient
set.seed(1121)
ari_ma <- arima.sim(n=len_gth, model=list(ar=0.01))
tseries::adf.test(ari_ma)
# Simulate arima with large AR coefficient
set.seed(1121)
ari_ma <- arima.sim(n=len_gth, model=list(ar=0.99))
tseries::adf.test(ari_ma)
# Simulate arima with different AR coefficients
coeff_s <- seq(0.99, 1.0, 0.001) - 0.001
set.seed(1121)
in_nov <- rnorm(len_gth)
adf_test <- sapply(coeff_s, function(co_eff) {
  ari_ma <- filter(x=in_nov, filter=co_eff, method="recursive")
  ad_f <- suppressWarnings(tseries::adf.test(ari_ma))
  c(adf_stat=unname(ad_f$statistic), pval=ad_f$p.value)
})  # end sapply
plot(x=coeff_s, y=adf_test["pval", ], main="ADF Pval versus AR coefficient",
     xlab="AR coefficient", ylab="ADF pval", t="l", col="blue", lwd=2)
plot(x=coeff_s, y=adf_test["adf_stat", ], main="ADF Stat versus AR coefficient",
     xlab="AR coefficient", ylab="ADF stat", t="l", col="blue", lwd=2)
      @
      \vspace{-1em}
      The \emph{ADF} test is weak in the sense that it requires a lot of data to identify a \emph{unit root} process.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Variance of Unit-root Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{AR(1)} process:
      $r_i = \varphi r_{i-1} + \xi_i$
      has the following characteristic equation:
      $1 - \varphi z = 0$,
      with a root equal to:
      $z = 1 / \varphi$
      \vskip1ex
      If $\varphi = 1$, then the characteristic equation has a \emph{unit root} (and therefore it isn't stationary), and the process follows:
      $r_i = r_{i-1} + \xi_i$
      \vskip1ex
      The above is called a \emph{Wiener} process (Brownian motion, random walk), and it's an example of a \emph{unit-root} process.
      \vskip1ex
      The expected value of the \emph{AR(1)} process $r_i = \varphi r_{i-1} + \xi_i$ is equal to zero: $\mathbb{E}[r_i] = \frac{\mathbb{E}[\xi_i]}{1 - \varphi} = 0$.
      \vskip1ex
      And its variance is equal to: $\sigma^2 = \mathbb{E}[r_i^2] = \frac{\sigma_{\xi}^2}{1 - \varphi^2}$.
      \vskip1ex
      If $\varphi = 1$, then the \emph{variance} grows over time and becomes infinite over time, so the process isn't stationary.
      \vskip1ex
      The variance of the \emph{Wiener} process $r_i = r_{i-1} + \xi$ is proportional to time: $\sigma_i^2 = \mathbb{E}[r_i^2] = i \sigma_{\xi}^2$
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/brownian_var.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate random walks using apply() loops
set.seed(1121)  # Initialize random number generator
rand_walks <- matrix(rnorm(1000*100), ncol=1000)
rand_walks <- apply(rand_walks, 2, cumsum)
vari_ance <- apply(rand_walks, 1, var)
# Simulate random walks using vectorized functions
set.seed(1121)  # Initialize random number generator
rand_walks <- matrixStats::colCumsums(matrix(rnorm(1000*100), ncol=1000))
vari_ance <- matrixStats::rowVars(rand_walks)
par(mar=c(5, 3, 2, 2), oma=c(0, 0, 0, 0))
plot(vari_ance, xlab="time steps", ylab="",
     t="l", col="blue", lwd=2,
     main="Variance of Random Walk")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelations of Autoregressive \protect\emph{AR} Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The autocorrelation $\rho_i$ of an \emph{AR(1)} process (defined as $r_i = \varphi r_{i-1} + \xi_i$), satisfies the recursive equation: $\rho_i = \varphi \rho_{i-1}$, with $\rho_1 = \varphi$.
      \vskip1ex
      Therefore \emph{AR(1)} processes have exponentially decaying autocorrelations: $\rho_i = \varphi^i$.
      \vskip1ex
      The \emph{AR(1)} process can be solved recursively:
      \begin{align*}
        r_1 &= \xi_1 \\
        r_2 &= \varphi r_1 + \xi_2 = \xi_2 + \varphi \xi_1 \\
        r_3 &= \xi_3 + \varphi \xi_2 + \varphi^2 \xi_1 \\
        r_4 &= \xi_4 + \varphi \xi_3 + \varphi^2 \xi_2 + \varphi^3 \xi_1
      \end{align*}
      Therefore the \emph{AR(1)} process can be expressed as a \emph{moving average} (\emph{MA}) of the \emph{innovations} $\xi_i$: $r_i = \sum_{i=1}^n {\varphi^{i-1} \xi_i}$.
      \vskip1ex
      If $\varphi < 1.0$ then the influence of the innovation $\xi_i$ decays exponentially.
      \vskip1ex
      If $\varphi = 1.0$ then the influence of the random innovations $\xi_i$ persists indefinitely, so that the variance of $r_i$ increases linearly with time.
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ar_acf.png}
      An \emph{AR(1)} process has an exponentially decaying \emph{ACF}.
      % \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=3.5)
par(mar=c(3, 3, 2, 1), oma=c(0, 0, 0, 0))
# Simulate AR(1) process
ari_ma <- arima.sim(n=729, model=list(ar=0.8))
# ACF of AR(1) process
ac_f <- rutils::plot_acf(ari_ma, lag=10,
  xlab="", ylab="",
  main="Autocorrelations of AR(1) process")
ac_f$acf[1:5]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Partial Autocorrelations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If two random variables are both correlated to a third variable, then they are indirectly correlated with each other.
      \vskip1ex
      The indirect correlation can be removed by defining new variables with no correlation to the third variable.
      \vskip1ex
      The \emph{partial correlation} is the correlation after the correlations to the common variables are removed.
      \vskip1ex
      The \emph{partial autocorrelations} $\varrho_i$ of an \emph{AR(1)} process can be computed recursively from the autocorrelations $\rho_i$ using the Durbin-Levinson algorithm:
      \begin{align*}
        \varrho_1 &= \rho_1 \\
        \varrho_2 &= \rho_2 - \varrho_1 \rho_1 \\
        \varrho_3 &= \rho_3 - \varrho_1 \rho_2 - \varrho_2 \rho_1
      \end{align*}
      The function \texttt{pacf()} calculates and plots the \emph{partial autocorrelations}, but it performs regressions instead of using the Durbin-Levinson algorithm.
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ar_pacf.png}
      % \vspace{-2em}
      An \emph{AR(1)} process has an exponentially decaying \emph{ACF} and a non-zero \emph{PACF} at lag one.
      <<echo=TRUE,eval=FALSE>>=
# PACF of AR(1) process
pac_f <- pacf(ari_ma, lag=10,
  xlab="", ylab="", main="")
title("Partial autocorrelations of AR(1) process",
  line=1)
pac_f <- drop(pac_f$acf)
pac_f[1:5]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Partial Autocorrelations of \protect\emph{AR(1)} Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An autocorrelation of lag $1$ induces higher order autocorrelations of lag \texttt{2, 3, ...}, which may obscure the true higher order autocorrelations.
      \vskip1ex
      A linear combination of the time series and its own lag can be created, such that its lag $1$ autocorrelation is zero.
      \vskip1ex
      The lag $2$ autocorrelation of this new series is called the \emph{partial autocorrelation} of lag $2$, and represents the true second order autocorrelation.
      \vskip1ex
      The \emph{partial autocorrelation} of lag \texttt{k} is the autocorrelation of lag \texttt{k}, after all the autocorrelations of lag \texttt{1, ..., k-1} have been removed.
      \vskip1ex
      The \emph{partial autocorrelations} $\varrho_i$ of an \emph{AR(1)} process can be computed recursively from the autocorrelations $\rho_i$ using the Durbin-Levinson algorithm:
      \begin{displaymath}
        \varrho_k = \rho_k - \sum_{i=1}^{k-1} {\varrho_i \rho_{k-i}}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Compute pacf recursively from acf
ac_f <- rutils::plot_acf(ari_ma, lag=10, plo_t=FALSE)
ac_f <- drop(ac_f$acf)
pac_f <- numeric(3)
pac_f[1] <- ac_f[1]
pac_f[2] <- ac_f[2] - ac_f[1]^2
pac_f[3] <- ac_f[3] -
  pac_f[2]*ac_f[1] - ac_f[2]*pac_f[1]
# Compute pacf recursively in a loop
pac_f <- numeric(NROW(ac_f))
pac_f[1] <- ac_f[1]
for (it in 2:NROW(pac_f)) {
  pac_f[it] <- ac_f[it] -
    pac_f[1:(it-1)] %*% ac_f[(it-1):1]
}  # end for
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Higher Order Autocorrelations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{AR}(3) process of order \emph{three} is defined by the formula:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \varphi_3 r_{i-3} + \xi_i
      \end{displaymath}
      Autoregressive processes \emph{AR(p)} of order \emph{p} have an exponentially decaying \emph{ACF} and a non-zero \emph{PACF} up to lag \emph{p}.
      <<ar_pacf,echo=(-(1:2)),eval=FALSE,fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # Set plot panels
# Simulate AR process of returns
ari_ma <- arima.sim(n=729,
  model=list(ar=c(0.1, 0.5, 0.1)))
# ACF of AR(3) process
rutils::plot_acf(ari_ma, lag=10, xlab="", ylab="",
         main="ACF of AR(3) process")
# PACF of AR(3) process
pacf(ari_ma, lag=10, xlab="", ylab="",
     main="PACF of AR(3) process")
      @
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/ar_pacf-1}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Calibrating Time Series Models}


%%%%%%%%%%%%%%%
\subsection{draft: Calibrating Autoregressive Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)} defined as:
      \begin{multline*}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i = \\
        \sum_{j=1}^{p} {\varphi_j r_{i-j}} + \xi_i
      \end{multline*}
      Can be expressed as a \emph{multivariate} linear regression model, with the \emph{response} equal to $r_i$, and the columns of the \emph{design matrix} equal to the lags of $r_i$.
      \vskip1ex
      The function \texttt{ar.ols()} from the base package \emph{stats} calibrates (fits) an \emph{AR(p)} model to a univariate time series, using the \emph{maximum likelihood} method (which may give slightly different coefficients than the linear regression model).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate AR process using filter()
len_gth <- 1e3
co_eff <- c(0.1, 0.39, 0.5)
set.seed(1121); in_nov <- rnorm(len_gth)
ari_ma <- as.numeric(filter(x=in_nov,
  filter=co_eff, method="recursive"))
# Calibrate AR model using ar.ols()
ar_fit <- ar.ols(ari_ma)
class(ar_fit)
is.list(ar_fit)
drop(ar_fit$ar)

# wippp
# Calibrate AR model using regression
# Define design matrix with intercept column
de_sign <- sapply(1:3, function(lagg) {
  rutils::lag_it(ari_ma, lagg=lagg)
})  # end sapply
de_sign <- cbind(rep(1, len_gth), de_sign)
# Regression coefficients with response equal to ari_ma
design_inv <- MASS::ginv(de_sign)
coeff_reg <- drop(design_inv %*% ari_ma)
all.equal(drop(ar_fit$ar), coeff_reg[-1], check.attributes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Calibrating Autoregressive Models Using Maximum Likelihood}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)} defined as:
      \begin{multline*}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i = \\
        \sum_{j=1}^{p} {\varphi_j r_{i-j}} + \xi_i
      \end{multline*}
      Can be expressed as a \emph{multivariate} linear regression model, with the \emph{response} equal to $r_i$, and the columns of the \emph{design matrix} equal to the lags of $r_i$.
      \vskip1ex
      The function \texttt{arima()} from the base package \emph{stats} calibrates (fits) an \emph{ARIMA} model to a univariate time series, using the \emph{maximum likelihood} method (which may give slightly different coefficients than the linear regression model).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate AR process using filter()
len_gth <- 1e3
co_eff <- c(0.1, 0.39, 0.5)
set.seed(1121); in_nov <- rnorm(len_gth)
ari_ma <- as.numeric(filter(x=in_nov,
  filter=co_eff, method="recursive"))
# Calibrate ARIMA model using arima()
arima_fit <- arima(ari_ma,
  order=c(3,0,0), include.mean=FALSE)

# wippp
# Calibrate ARIMA model using regression
# Define design matrix
ari_ma <- (ari_ma - mean(ari_ma))
de_sign <- sapply(1:3, function(lagg) {
  rutils::lag_it(ari_ma, lagg=lagg)
})  # end sapply
# Calculate de-meaned re_turns matrix
de_sign <- t(t(de_sign) - colMeans(de_sign))
design_inv <- MASS::ginv(de_sign)
# Regression coefficients with response equal to ari_ma
coeff_reg <- drop(design_inv %*% ari_ma)

all.equal(arima_fit$coef, coeff_reg, check.attributes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Standard Errors of the \protect\emph{ARIMA} Coefficients}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)} defined as:
      \begin{multline*}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i = \\
        \sum_{j=1}^{p} {\varphi_j r_{i-j}} + \xi_i
      \end{multline*}
      Can be expressed as a \emph{multivariate} linear regression model, with the \emph{response} equal to $r_i$, and the columns of the \emph{design matrix} equal to the lags of $r_i$.
      \vskip1ex
      The function \texttt{arima()} from the base package \emph{stats} calibrates (fits) an \emph{ARIMA} model to a univariate time series, using the \emph{maximum likelihood} method (which may give slightly different coefficients than the linear regression model).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate fitted values from regression coefficients
fit_ted <- drop(de_sign %*% coeff_reg)
all.equal(fit_ted, mod_el$fitted.values, check.attributes=FALSE)
# Calculate the residuals
resid_uals <- drop(ari_ma - fit_ted)
all.equal(resid_uals, mod_el$residuals, check.attributes=FALSE)
# The residuals have zero mean
all.equal(sum(resid_uals), target=0)
# The residuals are orthogonal to the de_sign columns (predictors)
sapply(resid_uals %*% de_sign,
       all.equal, target=0)
# The residuals are orthogonal to the fitted values
all.equal(sum(resid_uals*fit_ted), target=0)

# Variance of residuals
var_resid <- sum(resid_uals^2)/(len_gth - NROW(coeff_reg))

# Design matrix squared
design_2 <- crossprod(de_sign)
# design_2 <- t(de_sign) %*% de_sign
# Calculate covariance matrix of betas
co_var <- var_resid*MASS::ginv(design_2)
# Round(co_var, 3)
coeff_regd <- sqrt(diag(co_var))
all.equal(coeff_regd, arima_fit$var.coef, check.attributes=FALSE)

# Calculate t-values of betas
beta_tvals <- drop(coeff_reg)/coeff_regd




# Simulate AR process using filter()
len_gth <- 1e3
co_eff <- c(0.1, 0.39, 0.5)
set.seed(1121); in_nov <- rnorm(len_gth)
ari_ma <- as.numeric(filter(x=in_nov,
  filter=co_eff, method="recursive"))
# wippp
# Calibrate ARIMA model using regression
# Define design matrix
de_sign <- sapply(1:3, function(lagg) {
  rutils::lag_it(ari_ma, lagg=lagg)
})  # end sapply
design_inv <- MASS::ginv(de_sign)
# Regression coefficients with response equal to ari_ma
coeff_reg <- drop(design_inv %*% ari_ma)
# Calibrate ARIMA model using arima()
arima_fit <- arima(ari_ma,
  order=c(3,0,0), include.mean=FALSE)
arima_fit$coef
all.equal(arima_fit$coef, coeff_reg, check.attributes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


% wippp: add order selection and AIC
%%%%%%%%%%%%%%%
\subsection{draft: \protect\emph{AR(p)} Order Selection Using Information Criteria}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Order selection means determining the order of the \emph{ARIMA} model that best fits the time series.
      \vskip1ex
      Calibrating an \emph{ARIMA} model is a two-step process: first determine the order of the \emph{ARIMA} model, and then calculate the coefficients.
      \vskip1ex
      The function \texttt{auto.arima()} from the package \emph{forecast} performs order selection, and calibrates an \emph{ARIMA} model to a univariate time series.
      \vskip1ex
      The function \texttt{arima()} from the base package \emph{stats} fits an \emph{ARIMA} model to a univariate time series.
      \vskip1ex
      The function \texttt{auto.arima()} from the package \emph{forecast} automatically calibrates an \emph{ARIMA} model to a univariate time series.
      \vskip1ex
      An \emph{autoregressive} process \emph{AR(p)} defined as:
      \begin{multline*}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i = \\
        \sum_{j=1}^{p} {\varphi_j r_{i-j}} + \xi_i
      \end{multline*}
      Can be solved as a \emph{multivariate} linear regression model, with the \emph{response} equal to $r_i$, and the \emph{design matrix} columns equal to the lags of $r_i$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calibrate ARIMA model using auto.arima()
# library(forecast)  # Load forecast
forecast::auto.arima(ari_ma, max.p=3, max.q=0, max.d=0)
# Calibrate ARIMA model using arima()
arima_fit <- arima(ari_ma,
  order=c(3,0,0), include.mean=FALSE)
arima_fit$coef
# Calibrate ARIMA model using auto.arima()
# library(forecast)  # Load forecast
forecast::auto.arima(ari_ma, max.p=3, max.q=0, max.d=0)
# Calibrate ARIMA model using regression
ari_ma <- as.numeric(ari_ma)
# Define design matrix
de_sign <- sapply(1:3, function(lagg) {
  rutils::lag_it(ari_ma, lagg=lagg)
})  # end sapply
# Generalized inverse of design matrix
design_inv <- MASS::ginv(de_sign)
# Regression coefficients with response equal to ari_ma
co_eff <- drop(design_inv %*% ari_ma)
all.equal(arima_fit$coef, co_eff, check.attributes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Yule-Walker Equations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      To lighten the notation we can assume that the time series $r_i$ has zero mean $\mathbb{E}[r_i] = 0$ and unit variance $\mathbb{E}[r_i^2] = 1$.  ($\mathbb{E}$ is the expectation operator.)
      \vskip1ex
      Then the \emph{autocorrelations} of $r_i$ are equal to: $\rho_k = \mathbb{E}[r_i r_{i-k}]$.
      \vskip1ex
      If we multiply the \emph{autoregressive} process \emph{AR(p)}: $r_i = \sum_{j=1}^{p} {\varphi_j r_{i-j}} + \xi_i$, by $r_{i-k}$ and take the expectations, then we obtain the Yule-Walker equations:
      \begin{displaymath}
        \begin{pmatrix}
          \rho_1 \\
          \rho_2 \\
          \rho_3 \\
          \vdots \\
          \rho_p
        \end{pmatrix} =
        \begin{pmatrix}
          1 & \rho_1 & \dots & \rho_{p-1} \\
          \rho_1 & 1 & \dots & \rho_{p-2} \\
          \rho_2 & \rho_1 & \dots & \rho_{p-3} \\
          \vdots & \vdots & \ddots & \vdots \\
          \rho_{p-1} & \rho_{p-2} & \dots & 1
        \end{pmatrix}
        \begin{pmatrix}
          \varphi_1 \\
          \varphi_2 \\
          \varphi_3 \\
          \vdots \\
          \varphi_p
        \end{pmatrix}
      \end{displaymath}
      The Yule-Walker equations relate the \emph{autocorrelation coefficients} $\rho_i$ with the coefficients of the \emph{AR(p)} process $\varphi_i$.
      \vskip1ex
      The Yule-Walker equations can be solved for the \emph{AR(p)} coefficients $\varphi_i$ using matrix inversion.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate ARIMA
ari_ma <- arima.sim(n=1e4, model=list(ar=c(0.2, 0.7)))
# Compute autocorrelation coefficients
ac_f <- acf(ari_ma, lag=10, plot=FALSE)
ac_f <- drop(ac_f$acf)
acf1 <- ac_f[-NROW(ac_f)]
# Define Yule-Walker matrix
yule_walker <- sapply(2:9, function(lagg) {
  c(acf1[lagg:1], acf1[2:(NROW(acf1)-lagg+1)])
})  # end sapply
yule_walker <- cbind(acf1, yule_walker, rev(acf1))
# Generalized inverse of Yule-Walker matrix
yule_walker_inv <- MASS::ginv(yule_walker)
# Solve Yule-Walker equations
co_eff <- drop(yule_walker_inv %*% ac_f[-1])
round(co_eff[1:5], 5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Forecasting Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)}:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Can be simulated using the function \texttt{filter()} with the argument \texttt{method="recursive"}.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate AR process using filter()
len_gth <- 1e2
co_eff <- c(0.1, 0.39, 0.5)
set.seed(1121); in_nov <- rnorm(len_gth)
ari_ma <- filter(x=in_nov,
  filter=co_eff, method="recursive")
ari_ma <- as.numeric(ari_ma)
      @
      \vspace{-1em}
      The one step ahead \emph{forecast} $f_i$ from the time series $r_i$ is equal to its \emph{convolution} with the \emph{AR(p)} coefficients:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Forecast AR(3) process using loop in R
forecast_s <- numeric(NROW(ari_ma))
forecast_s[1] <- 0
forecast_s[2] <- co_eff[1]*ari_ma[1]
forecast_s[3] <- co_eff[1]*ari_ma[2] +
  co_eff[2]*ari_ma[1]
for (it in 4:NROW(forecast_s)) {
  forecast_s[it] <- ari_ma[(it-1):(it-3)] %*% co_eff
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ar_forecast.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot with legend
plot(ari_ma,
  main="Forecasting Using AR(3) Model",
  xlab="", ylab="", type="l")
lines(forecast_s, col="orange", lwd=3)
legend(x="topright", legend=c("series","forecasts"),
       col=c("black", "orange"), lty=1, lwd=6,
       cex=0.9, bg="white", bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Forecasting Autoregressive Processes Using \texttt{filter()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The one step ahead \emph{forecast} $f_i$ from the time series $r_i$ is equal to its \emph{convolution} with the \emph{AR(p)} coefficients:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
      The above \emph{convolution} can be quickly calculated by using the function \texttt{filter()} with the argument \texttt{method="convolution"}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Forecast using filter()
forecasts_filter <- filter(x=ari_ma, sides=1,
  filter=co_eff, method="convolution")
forecasts_filter <- as.numeric(forecasts_filter)
# Compare with loop in R
all.equal(forecast_s[-(1:3)],
  forecasts_filter[-c(1:2, NROW(forecasts_filter))],
  check.attributes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Forecasting Autoregressive Processes Using \texttt{predict.Arima()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The forecasts of the \emph{AR(p)} process can also be calculated using the function \texttt{predict()}.
      \vskip1ex
      The function \texttt{predict()} is a \emph{generic function} for forecasting based on a given model.
      \vskip1ex
      The \emph{method} \texttt{predict.Arima()} is \emph{dispatched} by \texttt{R} for calculating predictions from \emph{ARIMA} models produced by the function \texttt{arima()}.
      \vskip1ex
      The \emph{method} \texttt{predict.Arima()} returns a prediction object which is a \texttt{list} containing the predicted value and its standard error.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calibrate ARIMA model using arima()
arima_fit <- arima(ari_ma, order=c(3,0,0),
                   include.mean=FALSE)
# One-step-ahead forecast using predict.Arima()
pre_dict <- predict(arima_fit, n.ahead=1)
# Or directly call predict.Arima()
# pre_dict <- predict.Arima(arima_fit, n.ahead=1)
# Inspect the prediction object
class(pre_dict)
names(pre_dict)
class(pre_dict$pred)
unlist(pre_dict)
# One-step-ahead forecast using matrix algebra
fore_cast <- drop(
  ari_ma[len_gth:(len_gth-2)] %*% arima_fit$coef)
# Compare one-step-ahead forecasts
all.equal(pre_dict$pred[[1]], fore_cast)
# Get information about predict.Arima()
?stats:::predict.Arima
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Forecasting Residuals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{residuals} $\varepsilon_i$ are equal to the differences between the actual values $r_i$ minus the \emph{forecasts} $f_i$: $\varepsilon_i = r_i - f_i$.
      \vskip1ex
      Accurate forecasting of an \emph{AR(p)} process requires knowing its coefficients.
      \vskip1ex
      If the coefficients of the \emph{AR(p)} process are known exactly, then its \emph{residuals} $\varepsilon_i$ are equal to its \emph{innovations} $\xi_i$: $\varepsilon_i = r_i - f_i = \xi_i$.
      \vskip1ex
      In reality, we don't know the \emph{AR(p)} coefficients, so they must be calibrated from an empirical time series.
      \vskip1ex
      If the \emph{AR(p)} coefficients are calibrated from an empirical time series, then its \emph{residuals} are \emph{not} equal to its \emph{innovations}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Compare residuals with innovations
residual_s <- (ari_ma - forecast_s)
all.equal(in_nov, residual_s,
          check.attributes=FALSE)
plot(residual_s, t="l", lwd=3, main="ARIMA Forecast Errors")
# Calibrate ARIMA model using arima()
arima_fit <- arima(ari_ma, order=c(3,0,0),
                   include.mean=FALSE)
coef_fit <- arima_fit$coef
# Forecast using from fitted coefficients
forecast_s <- numeric(NROW(ari_ma))
forecast_s[1] <- 0
forecast_s[2] <- coef_fit[1]*ari_ma[1]
forecast_s[3] <- coef_fit[1]*ari_ma[2] +
  coef_fit[2]*ari_ma[1]
for (it in 4:NROW(forecast_s)) {
  forecast_s[it] <-
    ari_ma[(it-1):(it-3)] %*% coef_fit
}  # end for
# Calculate the forecasting residuals
residual_s <- (ari_ma - forecast_s)
all.equal(in_nov, residual_s,
          check.attributes=FALSE)
tail(cbind(in_nov, residual_s))
      @
  \end{columns}
\end{block}

\end{frame}


% wippp: add Forecasting errors
%%%%%%%%%%%%%%%
\subsection{draft: The Standard Errors of Forecasts from Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The one step ahead \emph{forecast} $f_i$ of the time series $r_i$ using the process \emph{AR(p)} is defined as:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
      The function \texttt{filter()} with the argument \texttt{method="convolution"} calculates the convolution of a vector with a filter.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate AR process using filter()
len_gth <- 1e2
co_eff <- c(0.1, 0.39, 0.5)
set.seed(1121)
ari_ma <- filter(x=rnorm(len_gth),
  filter=co_eff, method="recursive")
ari_ma <- as.numeric(ari_ma)
# Forecast AR(3) process
forecast_s <- numeric(NROW(ari_ma))
forecast_s[2] <- co_eff[1]*ari_ma[1]
forecast_s[3] <- co_eff[1]*ari_ma[2] +
  co_eff[2]*ari_ma[1]
for (it in 4:NROW(forecast_s)) {
  forecast_s[it] <-
    ari_ma[(it-1):(it-3)] %*% co_eff
}  # end for
# Forecast using filter()
forecasts_filter <- filter(x=ari_ma, sides=1,
  filter=co_eff, method="convolution")
class(forecasts_filter)
all.equal(forecast_s[-(1:4)],
  forecasts_filter[-c(1:3, NROW(forecasts_filter))],
  check.attributes=FALSE)
# Compare residuals with innovations
residual_s <- (ari_ma-forecast_s)
tail(cbind(in_nov, residual_s))


# ari_ma <- as.numeric(lh)
# len_gth <- NROW(ari_ma)
# Compare one-step-ahead forecasts
# arima_fit <- arima(ari_ma, order=c(3,0,0), method="ML", include.mean=FALSE)

# Compare many one-step-ahead forecasts
forecast_s <- sapply(31:len_gth, function(x) {
  cat("len = ", x, "\n")
  # ari_ma <- filter(x=rnorm(len_gth+1), filter=co_eff, method="recursive")
  arima_fit <- arima(ari_ma[1:x], order=c(3,0,0), include.mean=FALSE)
  pre_dict <- predict(arima_fit, n.ahead=1)
  fore_cast <- drop(ari_ma[x:(x-2)] %*% arima_fit$coef)
  c(actual=ari_ma[x+1], forecast=fore_cast, predict=as.numeric(pre_dict$pred))
})  # end sapply
foo <- t(foo)
# hist(foo[, 1], breaks=30,
#   main="", ylim=c(0, 60), xlim=c(-0.04, 0.04),
#   xlab="", ylab="", freq=FALSE)

hist(foo[, 1], ylim=c(0, 0.15), freq=FALSE)
lines(density(foo[, 1]), col='blue', lwd=3)
lines(density(foo[, 2]), col='green', lwd=3)
lines(density(foo[, 3]), col='red', lwd=3)


# Forecast AR(3) process
forecast_s <- numeric(NROW(ari_ma))
forecast_s[2] <- co_eff[1]*ari_ma[1]
forecast_s[3] <- co_eff[1]*ari_ma[2] +
  co_eff[2]*ari_ma[1]
for (it in 4:NROW(forecast_s)) {
  forecast_s[it] <-
    ari_ma[(it-1):(it-3)] %*% co_eff
}  # end for
# Forecast using filter()
forecasts_filter <- filter(x=ari_ma, sides=1,
  filter=co_eff, method="convolution")
class(forecasts_filter)
all.equal(forecast_s[-(1:4)],
  forecasts_filter[-c(1:3, NROW(forecasts_filter))],
  check.attributes=FALSE)
# Compare residuals with innovations
residual_s <- (ari_ma-forecast_s)
tail(cbind(in_nov, residual_s))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ar_forecast.png}
      Accurate forecasting requires knowing the order $p$ of the \emph{AR(p)} process and its coefficients.
      <<echo=TRUE,eval=FALSE>>=
# Plot with legend
plot(ari_ma,
  main="Forecasting Using AR(3) Model",
  xlab="", ylab="", type="l")
lines(forecast_s, col="orange", lwd=3)
legend(x="topright", legend=c("series","forecasts"),
       col=c("black", "orange"), lty=1, lwd=6,
       cex=0.9, bg="white", bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting an Autoregressive Forecasting Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      \emph{Backtesting} is the simulation of a forecasting model on historical data to test its accuracy.
      \vskip1ex
      The autoregressive forecasting model can be \emph{backtested} by calculating forecasts over a rolling look-back interval.
      \vskip1ex
      The coefficients of the \emph{AR(p)} process are calculated using past data, and then applied for calculating out-of-sample forecasts.
    \column{0.6\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate AR process using filter()
co_eff <- c(0.1, 0.39, 0.5); n_coeff <- NROW(co_eff)
len_gth <- 1e3; set.seed(1121)
ari_ma <- as.numeric(filter(x=rnorm(len_gth),
  filter=co_eff, method="recursive"))
# Define design matrix
de_sign <- sapply(1:n_coeff, function(lagg) {
  rutils::lag_it(ari_ma, lagg=lagg)
})  # end sapply
de_sign <- cbind(rep(1, len_gth), de_sign)
de_sign <- cbind(ari_ma, de_sign)
# Perform rolling forecasting
look_back <- 100 # Length of look-back interval
forecast_s <- sapply(n_coeff:len_gth, function(now) {
  # Subset the design matrix
  star_t <- max(1, now-look_back+1)
  de_sign <- de_sign[star_t:now, ]
  # Calculate AR(3) coefficients
  design_inv <- MASS::ginv(de_sign[, -1])
  co_eff <- drop(design_inv %*% de_sign[, 1])
  # Calculate forecast
  de_sign[NROW(de_sign):(NROW(de_sign)-n_coeff+1), 1] %*% co_eff
})  # end sapply
forecast_s <- c(rep(forecast_s[1], n_coeff-1), forecast_s)
# Lag the forecasts to push them out-of-sample
forecast_s <- rutils::lag_it(forecast_s)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Accuracy of the Autoregressive Forecasting Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The accuracy of a forecasting model can be measured using the \emph{mean squared error} and the \emph{correlation}.
      \vskip1ex
      The mean squared error (\emph{MSE}) of a forecasting model is the average of the squared forecasting residuals $\varepsilon_i$, equal to the differences between the actual values $r_i$ minus the \emph{forecasts} $f_i$: $\varepsilon_i = r_i - f_i$:
      \begin{displaymath}
        \operatorname{MSE} = \frac{1}{n} \sum_{i=1}^n (r_i - f_i)^2
      \end{displaymath}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Mean squared error
mean((ari_ma - forecast_s)^2)
# Correlation
sum(forecast_s*ari_ma)
# Plot forecasting series with legend
plot(ari_ma, xlab="", ylab="", type="l",
  main="Rolling Forecasting Using AR(3) Model")
lines(forecast_s, col="orange", lwd=1)
legend(x="topright", legend=c("series","forecasts"),
       col=c("black", "orange"), lty=1, lwd=6,
       cex=0.9, bg="white", bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ar_forecast_rolling.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Function for the Forecasting Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The parameters of the \emph{backtesting} function include the order of the \emph{AR(p)} forecasting model ($p$) and the length of look-back interval (\texttt{look\_back}).
    \column{0.6\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define backtesting function
back_test <- function(ari_ma, n_coeff=2, look_back=11) {
  # Define design matrix
  de_sign <- sapply(1:n_coeff, function(lagg) {
    rutils::lag_it(ari_ma, lagg=lagg)
  })  # end sapply
  de_sign <- cbind(rep(1, len_gth), de_sign)
  de_sign <- cbind(ari_ma, de_sign)
  # Perform rolling forecasting
  forecast_s <- sapply(n_coeff:NROW(ari_ma),
    function(now) {
      # Subset the design matrix
      star_t <- max(1, now-look_back+1)
      de_sign <- de_sign[star_t:now, ]
      n_rows <- NROW(de_sign)
      # Calculate AR coefficients
      design_inv <- MASS::ginv(de_sign[, -1])
      co_eff <- drop(design_inv %*% de_sign[, 1])
      # Calculate forecast
      de_sign[n_rows:(n_rows-n_coeff+1), 1] %*% co_eff
    })  # end sapply
  forecast_s <- c(rep(forecast_s[1], n_coeff-1), forecast_s)
  # Lag the forecasts to push them out-of-sample
  forecast_s <- rutils::lag_it(forecast_s)
  sum(forecast_s*ari_ma)
}  # end back_test
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: \protect\emph{AR(p)} Forecasting Model Order and Accuracy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The accuracy of the \emph{AR(p)} forecasting model depends on its order $p$.
      \vskip1ex
      The two \emph{meta-parameters} can be chosen by maximizing the \emph{correlation} or minimizing the \emph{MSE} of the model forecasts in a \emph{backtest} simulation.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate AR process using filter()
len_gth <- 1e3
co_eff <- c(0.1, 0.39, 0.5)
n_coeff <- NROW(co_eff)
set.seed(1121)
ari_ma <- as.numeric(filter(x=rnorm(len_gth),
  filter=co_eff, method="recursive"))

foo <- sapply(2*(1:11), back_test, ari_ma=ari_ma, look_back=look_back)


# Plot forecasting series with legend
plot(foo, xlab="", ylab="", type="l",
  main="Rolling Forecasting Using AR(3) Model")
lines(forecast_s, col="orange", lwd=1)
legend(x="topright", legend=c("series","forecasts"),
       col=c("black", "orange"), lty=1, lwd=6,
       cex=0.9, bg="white", bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ar_forecast_rolling.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Ornstein-Uhlenbeck Process}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{Ornstein-Uhlenbeck} process, the returns $r_i$ are proportional to the difference between the equilibrium price $\mu$ minus the current price $p_i$:
      \begin{displaymath}
        r_i = p_i - p_{i-1} = \theta \, (\mu - p_{i-1}) + \sigma \, \xi_i
      \end{displaymath}
      Where the parameter $\theta$ is the strength of mean reversion, $\sigma$ is the volatility, and $\xi_i$ are random normal \emph{innovations} with zero mean and unit variance.
      \vskip1ex
      The \emph{Ornstein-Uhlenbeck} process for prices can be written as an \emph{AR(1)} process plus a drift:
      \begin{displaymath}
        p_i = \theta \, \mu + (1 - \theta ) \, p_{i-1} + \sigma \, \xi_i
      \end{displaymath}
      The \emph{Ornstein-Uhlenbeck} process cannot be simulated using the function \texttt{filter()} because of the drift term in its equation, and must be simulated using loops.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define Ornstein-Uhlenbeck parameters
eq_price <- 1.0; sigma_r <- 0.02
the_ta <- 0.01; len_gth <- 1000
drif_t <- the_ta*eq_price
theta_1 <- 1-the_ta
# Simulate Ornstein-Uhlenbeck process
in_nov <- sigma_r*rnorm(len_gth)
price_s <- numeric(len_gth)
price_s[1] <- in_nov[1]
for (i in 2:len_gth) {
  price_s[i] <- theta_1*price_s[i-1] +
    in_nov[i] + drif_t
}  # end for
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Ornstein-Uhlenbeck Process Warmup and Mean Reversion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Stationary \emph{AR(p)} time series are mean reverting to zero, while the \emph{Ornstein-Uhlenbeck} process is mean reverting to a non-zero equilibrium price $\mu$.
      \vskip1ex
      The \emph{Ornstein-Uhlenbeck} process needs a \emph{warmup period} before it reaches equilibrium.
      <<echo=TRUE,eval=FALSE>>=
plot(price_s, type="l",
     xlab="periods", ylab="prices",
     main="Ornstein-Uhlenbeck process")
legend("topright",
       title=paste(c(paste0("sigma_r = ", sigma_r),
                     paste0("eq_price = ", eq_price),
                     paste0("the_ta = ", the_ta)),
                   collapse="\n"),
       legend="", cex=0.8,
       inset=0.1, bg="white", bty="n")
abline(h=eq_price, col='red', lwd=2)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ou_proc.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Ornstein-Uhlenbeck Process Returns Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under the \emph{Ornstein-Uhlenbeck} process, the returns are negatively correlated to the lagged prices.
      <<echo=TRUE,eval=FALSE>>=
re_turns <- rutils::diff_it(price_s)
lag_price <- rutils::lag_it(price_s)
for_mula <- re_turns ~ lag_price
l_m <- lm(for_mula)
summary(l_m)
# Plot regression
plot(for_mula, main="OU Returns Versus Lagged Prices")
abline(l_m, lwd=2, col="red")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ou_scatter.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating the Ornstein-Uhlenbeck Parameters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The volatility parameter of the Ornstein-Uhlenbeck process can be estimated directly from the returns.
      \vskip1ex
      The $\theta$ and $\mu$ parameters can be estimated from the linear regression of the returns versus the lagged prices.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# volatility parameter
c(sigma_r, sd(re_turns))
# Extract OU parameters from regression
co_eff <- summary(l_m)$coefficients
# theta strength of mean reversion
round(co_eff[2, ], 3)
# Equilibrium price
co_eff[1, 1]/co_eff[2, 1]
# Parameter and t-values
co_eff <- cbind(c(the_ta*eq_price, the_ta),
  co_eff[, 1:2])
rownames(co_eff) <- c("drift", "theta")
round(co_eff, 3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Schwartz Process}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Ornstein-Uhlenbeck} time series can have negative values, while prices are usually not negative.
      \vskip1ex
      The \emph{Schwartz} process is similar to the \emph{Ornstein-Uhlenbeck} process, but it avoids negative prices by using the percentage returns $\mathrm{d} \log{P}$ instead of the simple returns $\mathrm{d} P$:
      \begin{displaymath}
        r_i = \log{p_i} - \log{p_{i-1}} = \theta \, (\mu - p_{i-1}) + \sigma \, \xi_i
      \end{displaymath}
      Where the parameter $\theta$ is the strength of mean reversion, $\sigma$ is the volatility, and $\xi_i$ are random normal \emph{innovations} with zero mean and unit variance.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate Schwartz process
re_turns <- numeric(len_gth)
price_s <- numeric(len_gth)
price_s[1] <- eq_price
set.seed(1121)  # Reset random numbers
for (i in 2:len_gth) {
  re_turns[i] <- the_ta*(eq_price - price_s[i-1]) +
    sigma_r*rnorm(1)
  price_s[i] <- price_s[i-1] * exp(re_turns[i])
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ou_lognormal.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
plot(price_s, type="l",
     xlab="periods", ylab="prices",
     main="Log-normal Ornstein-Uhlenbeck process")
legend("topright",
       title=paste(c(paste0("sigma_r = ", sigma_r),
                     paste0("eq_price = ", eq_price),
                     paste0("the_ta = ", the_ta)),
                   collapse="\n"),
       legend="", cex=0.8,
       inset=0.12, bg="white", bty="n")
abline(h=eq_price, col='red', lwd=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Solution of the Ornstein-Uhlenbeck Equation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
    % wipp
      Thierfelder Ornstein-Uhlenbeck Process.pdf
      \vskip1ex
      The Standard \emph{Ornstein-Uhlenbeck} process has a solution in the form of an integral over \emph{Brownian motion}:
      \begin{flalign*}
        & \mathrm{d} \log{P_t} = \theta \, ( \mu - P_t ) \mathrm{d} t + \sigma \, \mathrm{d} W_t \\
        & \mathrm{d} \log{P_t} = \theta \, ( \mu - P_t ) \mathrm{d} t + \sigma \, \mathrm{d} W_t \\
        & \mathrm{d} \log{P_t} = \theta \, ( \mu - P_t ) \mathrm{d} t + \sigma \, \mathrm{d} W_t \\
      \end{flalign*}
      % \begin{displaymath}
      % \end{displaymath}
      Where $\theta$ is the strength of mean reversion, and $\sigma$ is the volatility.
      \vskip1ex
      The \emph{Ornstein-Uhlenbeck} process must be simulated using a \texttt{for()} loop, since it is path-dependent.
      <<echo=TRUE,eval=FALSE>>=
# Define Ornstein-Uhlenbeck parameters
eq_price <- 5.0; sigma_r <- 0.01
the_ta <- 0.01; len_gth <- 1000
# Simulate Ornstein-Uhlenbeck process
re_turns <- numeric(len_gth)
price_s <- numeric(len_gth)
price_s[1] <- 5.0
set.seed(1121)  # Reset random numbers
for (i in 2:len_gth) {
  re_turns[i] <- the_ta*(eq_price - price_s[i-1]) +
    sigma_r*rnorm(1)
  price_s[i] <- price_s[i-1] * exp(re_turns[i])
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ou_lognormal.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
plot(price_s, type="l",
     xlab="periods", ylab="prices",
     main="Ornstein-Uhlenbeck process")
legend("topright",
       title=paste(c(paste0("sigma_r = ", sigma_r),
                     paste0("eq_price = ", eq_price),
                     paste0("the_ta = ", the_ta)),
                   collapse="\n"),
       legend="", cex=0.8,
       inset=0.1, bg="white", bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


\end{document}
